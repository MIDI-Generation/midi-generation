{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mdZlKX0m42z3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random\n",
        "\n",
        "REST_ID = 128\n",
        "BOS_ID = 129\n",
        "EOS_ID = 130\n",
        "\n",
        "class MIDIVAE(nn.Module):\n",
        "  def __init__(self, encoder, decoder,classifier,max_length=502,device = 'cuda',kld_weight = 1.0, classifier_weight = 1.0):\n",
        "      super(MIDIVAE,self).__init__()\n",
        "\n",
        "      # untrained components\n",
        "      self.encoder = encoder\n",
        "      self.decoder = decoder  # Cross-Attention transformer to generate MIDI vectors\n",
        "      self.max_length = max_length\n",
        "      # pre-trained classifier\n",
        "      self.classifier = classifier\n",
        "      self.device = device\n",
        "\n",
        "      #constants\n",
        "      self.kld_weight = kld_weight\n",
        "      self.classifier_weight = classifier_weight\n",
        "      self.BOS_ID = BOS_ID\n",
        "      self.EOS_ID = EOS_ID\n",
        "      self.REST_ID = REST_ID\n",
        "\n",
        "  def forward(self, x, label, teacher_forcing_ratio=0.9):\n",
        "        # Encoder pass\n",
        "        z, mean, logvar = self.encoder(x,label)\n",
        "\n",
        "        # Prepare decoder inputs\n",
        "        memory = self._prepare_memory(z)  # Could be latent projection\n",
        "\n",
        "        if self.training and random.random() < teacher_forcing_ratio:\n",
        "            # Teacher forcing - use ground truth as input\n",
        "            recon_midi = self.decoder(\n",
        "                tgt=x,\n",
        "                memory=memory,\n",
        "                z=z,\n",
        "                label=label,\n",
        "                teacher_forcing=True\n",
        "            )\n",
        "        else:\n",
        "            # Autoregressive generation\n",
        "            recon_midi = self.decoder.generate(\n",
        "                memory=memory,\n",
        "                z=z,\n",
        "                label=label,\n",
        "                max_len=self.max_length,\n",
        "                bos_id=self.BOS_ID,\n",
        "                eos_id=self.EOS_ID\n",
        "            )\n",
        "\n",
        "        # Classifier uses latent space\n",
        "        composer_pred = self.classifier(z)\n",
        "\n",
        "        return recon_midi, mean, logvar, composer_pred\n",
        "\n",
        "  def train_model(self, dataloader, optimizer, epochs=10):\n",
        "        self.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for x, label, composer in dataloader:\n",
        "                x, label, composer = x.to(self.device), label.to(self.device), composer.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                recon_x, mu, logvar, pred = self(x, label)\n",
        "\n",
        "                # Loss calculations\n",
        "                kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "                recon_loss = F.cross_entropy(\n",
        "                    recon_x.transpose(1, 2),  # [batch, vocab_size, seq_len]\n",
        "                    x  # [batch, seq_len]\n",
        "                )\n",
        "                cls_loss = F.cross_entropy(pred, composer)\n",
        "\n",
        "                loss = recon_loss + self.kld_weight*kld + self.classifier_weight*cls_loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch {epoch+1} | Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "  def _prepare_memory(self, z):\n",
        "        ## not sure if we'll need to format it a specific way\n",
        "        return z.unsqueeze(0)  # adjust based on your needs\n",
        "\n",
        "  def generate(self, z, label, temperature=1.0):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            memory = self._prepare_memory(z)\n",
        "            return self.decoder.generate(\n",
        "                memory=memory,\n",
        "                z=z,\n",
        "                label=label,\n",
        "                max_len=self.max_length,\n",
        "                temperature=temperature,\n",
        "                bos_id=self.bos_id,\n",
        "                eos_id=self.eos_id\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rSBeiQ44HXNq"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Token_Embedding(nn.Module):\n",
        "  def __init__(self,vocab_size,embedding_dim):\n",
        "    super(Token_Embedding,self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.embedding(x)\n",
        "\n",
        "class Pos_Embedding(nn.Module):\n",
        "  def __init__(self,max_len,embedding_dim):\n",
        "    super(Pos_Embedding,self).__init__()\n",
        "    self.pos_embedding = nn.Embedding(max_len,embedding_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    seq_len = x.size(1)\n",
        "    pos_ids = torch.arange(seq_len,device=x.device).unsqueeze(0)\n",
        "    return self.pos_embedding(pos_ids)\n",
        "\n",
        "class Transformer_Encoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, num_layers, ff_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=embedding_dim,\n",
        "                nhead=num_heads,\n",
        "                dim_feedforward=ff_dim,\n",
        "                batch_first=True  # ← Critical for your input shape\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class LatentSpace_Mean_Log(nn.Module):\n",
        "  def __init__(self,embedding_dim,latent_dim):\n",
        "    super(LatentSpace_Mean_Log,self).__init__()\n",
        "    self.fc_mu = nn.Linear(embedding_dim,latent_dim)\n",
        "    self.fc_logvar = nn.Linear(embedding_dim,latent_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    mu = self.fc_mu(x)\n",
        "    logvar = self.fc_logvar(x)\n",
        "\n",
        "    return mu,logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Wn_47aTe66-3"
      },
      "outputs": [],
      "source": [
        "class Variational_Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_len=502, latent_dim=64,\n",
        "                 num_heads=8, num_layers=6, ff_dim=512, label_dim=0):\n",
        "        super().__init__()\n",
        "        assert max_len >= 502, \"max_len must cover BOS+EOS+500 tokens\"\n",
        "\n",
        "        self.token_embedding = Token_Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_embedding = Pos_Embedding(max_len, embedding_dim)\n",
        "        self.encoder = Transformer_Encoder(embedding_dim, num_heads, num_layers, ff_dim)\n",
        "        self.latent_proj = LatentSpace_Mean_Log(embedding_dim, latent_dim)\n",
        "\n",
        "        # Optional label conditioning\n",
        "        self.label_proj = nn.Linear(label_dim, embedding_dim) if label_dim > 0 else None\n",
        "\n",
        "    def forward(self, x, label=None):\n",
        "        # Input x: [batch_size, 502]\n",
        "        tok_emb = self.token_embedding(x)  # [B, 502, D]\n",
        "        pos_emb = self.pos_embedding(x)    # [B, 502, D]\n",
        "        embeddings = tok_emb + pos_emb\n",
        "\n",
        "        # Inject label info (if provided)\n",
        "        if self.label_proj and label is not None:\n",
        "            label_emb = self.label_proj(label).unsqueeze(1)  # [B, 1, D]\n",
        "            embeddings += label_emb.expand(-1, x.size(1), -1)  # [B, 502, D]\n",
        "\n",
        "        # Transformer process\n",
        "        output = self.encoder(embeddings)  # [B, 502, D]\n",
        "\n",
        "        # Pool and project to latent space\n",
        "        pooled = output.mean(dim=1)  # [B, D]\n",
        "        mu, logvar = self.latent_proj(pooled)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "\n",
        "        return z, mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MTfiJo80fvcn"
      },
      "outputs": [],
      "source": [
        "#!!! Adapted from HW5!!!\n",
        "\n",
        "from torch import nn, Tensor\n",
        "import torch\n",
        "import math\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Cross-Attention Transformer Decoder\n",
        "class Transformer_Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 d_model: int,\n",
        "                 nhead: int,\n",
        "                 d_hid: int,\n",
        "                 nlayers: int,\n",
        "                 latent_dim: int,\n",
        "                 label_dim: int,\n",
        "                 dropout: float = 0.5,\n",
        "                 device = 'cuda'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.device = device\n",
        "        # discrete latent tokens\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        # predicted MIDI tokens\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        # Decoder (self-attention and cross-attention)\n",
        "        dec_layer = TransformerDecoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_decoder = TransformerDecoder(dec_layer, nlayers)\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        #weight modulation layers\n",
        "        self.z_scale = nn.Linear(latent_dim, d_model)\n",
        "        self.z_shift = nn.Linear(latent_dim, d_model)\n",
        "        # label dim should be B x\n",
        "        #label conditioning layers for cross attention\n",
        "        self.label_projection = nn.Linear(label_dim,d_model)\n",
        "        self.label_attention = nn.MultiheadAttention(d_model,nhead,dropout=dropout)\n",
        "        # Project z to scaling factors\n",
        "        self.init_weights()\n",
        "\n",
        "    def autoregressive_embed(self,tgt:Tensor,z:Tensor,label:Tensor) -> Tensor:\n",
        "      # goal is to generate things autoregressively, so it predicts the next\n",
        "      # token\n",
        "      # scale and embed target sequence and add pos encoding\n",
        "      tgt = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "      tgt = self.pos_encoder(tgt)\n",
        "      # weight modulation\n",
        "      shift = self.z_shift(z)\n",
        "      scale = self.z_scale(z)\n",
        "      tgt = tgt * scale.unsqueeze(0) + shift.unsqueeze(0)\n",
        "      # cross attention for label conditioning\n",
        "      label_emb = self.label_proj(label).unsqueeze(0)\n",
        "      tgt= self.label_attn(query=tgt_emb,key=label_emb,value=label_emb)[0]\n",
        "\n",
        "      return tgt\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.src_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.tgt_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, tgt: Tensor,memory: Tensor,z : Tensor, label:Tensor,teacher_forcing=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          tgt: [tgt_seq_len, batch_size] - the target sequence to predict the\n",
        "          next token of (discrete MIDI tokens) memory: [src_seq_len, batch_size]\n",
        "           - the discrete VQ-VAE code indices\n",
        "        Returns:\n",
        "          [tgt_seq_len, batch_size, tgt_vocab_size] logits output for the next\n",
        "          token\n",
        "        \"\"\"\n",
        "        # Scale and Embed the source sequence\n",
        "        src = self.src_embedding(memory) * math.sqrt(self.d_model)\n",
        "        # Add positional encoding\n",
        "        src = self.pos_encoder(src)\n",
        "\n",
        "        # teacher forcing so that during training we can train well but also\n",
        "        # autoregressively predict during evaluation\n",
        "        if teacher_forcing:\n",
        "          # Scale and Embed the target sequence\n",
        "          tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "          # Add positional encoding\n",
        "          tgt_emb = self.pos_encoder(tgt)\n",
        "        else:\n",
        "          tgt_emb = self.autoregressive_embed(tgt,z,label)\n",
        "\n",
        "        # Feature-Wise Weight Modulation - meant to ensure unique generation for\n",
        "        # given label\n",
        "        shift = self.z_shift(z)\n",
        "        scale = self.z_scale(z)\n",
        "        #this alters the target b4 the transformer can see them, it should bias\n",
        "        #the model enough\n",
        "        tgt_emb = tgt_emb * scale.unsqueeze(0) + shift.unsqueeze(0)\n",
        "\n",
        "        # cross-attention for labelness\n",
        "        label_emb = self.label_proj(label).unsqueeze(0)\n",
        "        tgt_emb= self.label_attn(query=tgt_emb,key=label_emb,value=label_emb)[0]\n",
        "\n",
        "        # Create the mask\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(len(tgt_emb)).to(device)\n",
        "        # Pass them through the transformer\n",
        "        output = self.transformer_decoder(tgt=tgt_emb, memory=src, tgt_mask=tgt_mask)\n",
        "        # Apply the linear layer\n",
        "        output = self.linear(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def generate(self,memory,z,label,max_len=502,temperature = 1.0):\n",
        "      batch_size = z.size(0)\n",
        "      device = self.device\n",
        "\n",
        "      #start with <BOS> token\n",
        "      # 1. Start with [BOS] token for all sequences in batch\n",
        "      tgt = torch.tensor(BOS_IDX, device=device).repeat(batch_size, 1)  # [batch_size, 1]\n",
        "\n",
        "      for _ in range(max_len):\n",
        "          # 2. Forward pass (autoregressive mode)\n",
        "          logits = self(\n",
        "              tgt=tgt.transpose(0, 1),  # Transformer expects [seq_len, batch_size]\n",
        "              memory=memory,\n",
        "              z=z,\n",
        "              label=label,\n",
        "              teacher_forcing=False\n",
        "          )[-1, :, :]  # Only keep logits for last position\n",
        "\n",
        "          # 3. Sample next token\n",
        "          probs = F.softmax(logits / temperature, dim=-1)\n",
        "          next_token = torch.multinomial(probs, num_samples=1)  # [batch_size, 1]\n",
        "\n",
        "          # 4. Update sequence\n",
        "          tgt = torch.cat([tgt, next_token], dim=1)\n",
        "\n",
        "          # 5. Early stop if all sequences finish\n",
        "          if (next_token == EOS_IDX).all():\n",
        "              break\n",
        "\n",
        "      return tgt[:, 1:]  # Remove [BOS]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vqa0QC59r17",
        "outputId": "605f2716-c6d8-4cf5-c748-f4897f878977"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_directory = '/content/drive/MyDrive/maestro_token_sequences.csv'"
      ],
      "metadata": {
        "id": "6wuW-wFzEBts"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "def string_to_vector(seq_str):\n",
        "    return [int(x) for x in seq_str.split()]\n",
        "\n",
        "def prepare_data(file, augmentations=False, collaborations=False,split='train'):\n",
        "    # Read the data\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # only read train data\n",
        "    if(split != 'all'):\n",
        "      df = df[df['split'] == split].copy()\n",
        "\n",
        "    # Convert sequences to vectors\n",
        "    df['sequence_vector'] = df['sequence'].apply(string_to_vector)\n",
        "\n",
        "    # Apply filters\n",
        "    if (not collaborations):\n",
        "        df = df[~df['composer'].str.contains('/', na=False)].copy()\n",
        "\n",
        "    if (not augmentations):\n",
        "        df = df[df['transposition amount'] == 0].copy()\n",
        "\n",
        "    # Create labels (assuming composer_label_dict exists)\n",
        "    clean_df = df.copy()  # Final cleaned version\n",
        "    composer_list = sorted(list(set(clean_df['composer'])))  # Convert to sorted list for consistent ordering\n",
        "    num_composers = len(composer_list)\n",
        "    print(f\"Unique composers: {composer_list}\")\n",
        "    print(f\"Total composers: {num_composers}\")\n",
        "\n",
        "    # Create proper label dictionary\n",
        "    composer_label_dict = {composer: idx for idx, composer in enumerate(composer_list)}\n",
        "    index_composer_dict = {idx: composer for composer, idx in composer_label_dict.items()}\n",
        "    clean_df['label'] = clean_df['composer'].map(composer_label_dict)\n",
        "\n",
        "    # Select only the two columns we want\n",
        "    data = clean_df[['sequence_vector', 'label']].copy()\n",
        "\n",
        "    return data,composer_label_dict,index_composer_dict"
      ],
      "metadata": {
        "id": "9EDS3PnCFnQg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "class MIDI_Dataset(Dataset):\n",
        "  def __init__(self,sequences,labels):\n",
        "    self.sequences = sequences\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.sequences)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      # Convert to tensor directly (no padding needed)\n",
        "      seq = torch.tensor(self.sequences[idx], dtype=torch.long)\n",
        "\n",
        "      if self.labels is not None:\n",
        "          label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
        "          return seq, label\n",
        "      return seq\n",
        "def create_MIDI_Dataloaders(train_data, batch_size=32):\n",
        "\n",
        "  # Create datasets\n",
        "  train_dataset = MIDI_Dataset(\n",
        "      sequences=train_data['sequence_vector'].tolist(),\n",
        "      labels=train_data['label']\n",
        "  )\n",
        "\n",
        "  # Create dataloaders\n",
        "  train_loader = DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      pin_memory=True,\n",
        "      num_workers=4  # Parallel loading\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  return train_loader\n",
        ""
      ],
      "metadata": {
        "id": "FMCJfN0PiOuD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "sdooHH7WGrsY",
        "outputId": "2b6e43ef-bbd5-4325-f166-3d991084de3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique composers: ['Alban Berg', 'Alexander Scriabin', 'Anton Arensky', 'Antonio Soler', 'Carl Maria von Weber', 'Claude Debussy', 'César Franck', 'Domenico Scarlatti', 'Edvard Grieg', 'Felix Mendelssohn', 'Franz Liszt', 'Franz Schubert', 'Frédéric Chopin', 'George Enescu', 'George Frideric Handel', 'Henry Purcell', 'Isaac Albéniz', 'Jean-Philippe Rameau', 'Johann Pachelbel', 'Johann Sebastian Bach', 'Johannes Brahms', 'Joseph Haydn', 'Leoš Janáček', 'Ludwig van Beethoven', 'Mily Balakirev', 'Modest Mussorgsky', 'Muzio Clementi', 'Nikolai Medtner', 'Orlando Gibbons', 'Percy Grainger', 'Pyotr Ilyich Tchaikovsky', 'Robert Schumann', 'Sergei Rachmaninoff', 'Wolfgang Amadeus Mozart']\n",
            "Total composers: 34\n",
            "(438646, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'classifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-40ed92b9be1f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# classifier = load_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMIDIVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encoder initialized with:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Shared Parameters\n",
        "vocab_size = 130        # Number of unique MIDI tokens\n",
        "embedding_dim = 256     # Size of token embeddings\n",
        "max_len = 502           # Max sequence length (500 tokens + EOS + BOS)\n",
        "latent_dim = 128        # Latent space dimension\n",
        "num_heads = 8           # Number of attention heads\n",
        "num_layers = 6          # Number of transformer layers\n",
        "ff_dim = 512            # Feed-forward layer dimension\n",
        "dropout = 0.1           # Dropout rate\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Additional Decoder-specific Parameters\n",
        "label_dim = 55          # Dimension for label embeddings (# of composers)\n",
        "hidden_dim = 256        # Hidden dimension in decoder (d_hid in your code)\n",
        "\n",
        "# Prepare Data\n",
        "data,composer_to_label_map,inv_map = prepare_data(file_directory,augmentations=True)\n",
        "train_data = create_MIDI_Dataloaders(data)\n",
        "\n",
        "print(data.shape)\n",
        "# Initialize Encoder\n",
        "encoder = Variational_Encoder(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    max_len=max_len,\n",
        "    latent_dim=latent_dim,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    ff_dim=ff_dim\n",
        ").to(device)\n",
        "\n",
        "# Initialize Decoder\n",
        "decoder = Transformer_Decoder(\n",
        "    src_vocab_size=vocab_size,    # Same as encoder vocab size\n",
        "    tgt_vocab_size=vocab_size,    # Same unless you have different input/output vocabs\n",
        "    d_model=embedding_dim,        # Should match encoder's embedding_dim\n",
        "    nhead=num_heads,              # Same as encoder\n",
        "    d_hid=hidden_dim,             # Decoder-specific hidden dim\n",
        "    nlayers=num_layers,           # Same as encoder\n",
        "    latent_dim=latent_dim,        # Same as encoder\n",
        "    label_dim=label_dim,          # For composer/style conditioning\n",
        "    device=device,\n",
        "    dropout=dropout,\n",
        ").to(device)\n",
        "\n",
        "#load classifier\n",
        "# classifier = load_model\n",
        "\n",
        "model = MIDIVAE(encoder,decoder,classifier,max_len)\n",
        "\n",
        "print(\"Encoder initialized with:\")\n",
        "print(f\"- Vocab size: {vocab_size}\")\n",
        "print(f\"- Embedding dim: {embedding_dim}\")\n",
        "print(f\"- Latent dim: {latent_dim}\")\n",
        "print(f\"- {num_layers} layers with {num_heads} attention heads each\")\n",
        "\n",
        "print(\"\\nDecoder initialized with:\")\n",
        "print(f\"- Same vocab size: {vocab_size}\")\n",
        "print(f\"- Label embedding dim: {label_dim}\")\n",
        "print(f\"- Hidden dim: {hidden_dim}\")\n",
        "print(f\"- Using dropout: {dropout}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TF415ubMPILE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}