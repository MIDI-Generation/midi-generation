{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "mdZlKX0m42z3"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "REST_ID = 128\n",
    "BOS_ID = 129\n",
    "EOS_ID = 130\n",
    "\n",
    "class DWA:\n",
    "    def __init__(self, T=2.0):\n",
    "        self.T = T  # temperature (smooths the adjustment)\n",
    "        self.loss_hist = []  # store past two epochs\n",
    "\n",
    "    def update(self, losses):\n",
    "        self.loss_hist.append(losses)\n",
    "        if len(self.loss_hist) > 2:\n",
    "            self.loss_hist.pop(0)\n",
    "\n",
    "    def get_weights(self):\n",
    "        if len(self.loss_hist) < 2:\n",
    "            return [1.0, 1.0, 1.0]  # equal at beginning\n",
    "        \n",
    "        w = []\n",
    "        for i in range(3):\n",
    "            r = self.loss_hist[1][i] / (self.loss_hist[0][i] + 1e-8)\n",
    "            w.append(r)\n",
    "        \n",
    "        w = torch.exp(torch.tensor(w) / self.T)\n",
    "        weights = (3 * w / w.sum()).tolist()  # 3 tasks here\n",
    "        return weights\n",
    "\n",
    "class MIDIVAE(nn.Module):\n",
    "  def __init__(self, encoder, decoder,classifier,max_length=502,device = 'cuda',kld_weight = 1.0, classifier_weight = 1.0):\n",
    "      super(MIDIVAE,self).__init__()\n",
    "\n",
    "      self.device = device\n",
    "      # untrained components\n",
    "      self.encoder = encoder.to(self.device)\n",
    "      self.decoder = decoder.to(self.device)  # Cross-Attention transformer to generate MIDI vectors\n",
    "      self.max_length = max_length\n",
    "      #workaround for cross-attention\n",
    "      self.num_memory_tokens = (encoder.latent_dim // 16)\n",
    "      self.memory_proj = nn.Sequential(\n",
    "            nn.Linear(encoder.latent_dim, decoder.d_model * self.num_memory_tokens),\n",
    "            nn.Unflatten(1, (self.num_memory_tokens, decoder.d_model))  # [B, num_tokens, d_model]\n",
    "        ).to(self.device)\n",
    "      # pre-trained classifier\n",
    "      self.classifier = classifier\n",
    "\n",
    "      #constants\n",
    "      self.kld_weight = kld_weight\n",
    "      self.classifier_weight = classifier_weight\n",
    "      self.BOS_ID = BOS_ID\n",
    "      self.EOS_ID = EOS_ID\n",
    "      self.REST_ID = REST_ID\n",
    "\n",
    "  \n",
    "  def forward(self, x, label, teacher_forcing_ratio=0.5):\n",
    "        z, mean, logvar = self.encoder(x, label)\n",
    "        memory = self._prepare_memory(z.to(self.device)).to(self.device)\n",
    "\n",
    "        tgt = torch.full((1, x.shape[0]), self.BOS_ID, device=self.device)  # [1, batch_size]\n",
    "        generated_tokens = tgt\n",
    "        #print(f'generated tok shape { generated_tokens.shape}')\n",
    "        logits_seq = []\n",
    "\n",
    "        # --- TRAINING PATH (teacher forcing) ------------------------------------\n",
    "        if self.training:\n",
    "            tgt_in  = torch.cat([tgt, x.T[:-1]], dim=0)   # [T, B] <- prepend <BOS>\n",
    "            logits  = self.decoder(                       # [B, T, V] (batch_first)\n",
    "                         tgt=tgt_in,                    # give decoder [B, T]\n",
    "                         memory=memory, z=z, label=label,\n",
    "                         teacher_forcing=True)\n",
    "            recon_logits = logits                        # keep for CE loss\n",
    "            recon_tokens = x.T                           # ground-truth tokens\n",
    "            composer_pred = self.classifier(x)           # use GT for classifier\n",
    "            return recon_logits, recon_tokens, mean, logvar, composer_pred\n",
    "        \n",
    "  def train_model(self, dataloader, optimizer, epochs=10,start_forcing_ratio=0.9,decay_factor=0.85):\n",
    "        dwa = DWA() #dynamic balancing for losses\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            forcing_ratio = start_forcing_ratio * (decay_factor ** epoch)\n",
    "            epoch_losses = [0.0, 0.0, 0.0]\n",
    "            total_loss = 0\n",
    "            loss_weights = dwa.get_weights()\n",
    "            for x, label in tqdm(dataloader,desc = f\"Training Epoch {epoch + 1}\"):\n",
    "                x, label = x.to(self.device), label.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                recon_logits, targets, mu, logvar, pred = self(x, label)\n",
    "                \n",
    "                # Loss calculations\n",
    "                kld_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                recon_loss = F.cross_entropy(recon_logits.reshape(-1, recon_logits.size(-1)),targets.reshape(-1))\n",
    "                cls_loss = F.cross_entropy(pred, label)\n",
    "                loss = loss_weights[0] * recon_loss + loss_weights[1] * kld_loss + loss_weights[2] * cls_loss\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_losses[0] += recon_loss.item()\n",
    "                epoch_losses[1] += kld_loss.item()\n",
    "                epoch_losses[2] += cls_loss.item()\n",
    "      \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "            dwa.update(epoch_losses)\n",
    "            print(f\"Epoch {epoch+1} | Loss: {total_loss/len(dataloader):.4f} | KLD: {kld_loss/len(dataloader)} | CLS: {cls_loss/len(dataloader)}\")\n",
    "\n",
    "            # Checkpoint: Save the model to a temporary directory after each epoch \n",
    "            # in case the model doesn't finish training during the 12 hour SCC secession\n",
    "            torch.save(model.state_dict(), f\"/projectnb/ec523/projects/proj_MIDIgen/checkpoints/MIDIVAE_v2_CHECKPOINT_{epoch+1}.pth\")\n",
    "  \n",
    "  def _prepare_memory(self, z):\n",
    "        memory = self.memory_proj(z).to(self.device)  # [B, num_tokens, d_model]\n",
    "        return memory.transpose(0, 1).to(self.device)\n",
    "\n",
    "  def generate(self, z, label, temperature=1.0):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            memory = self._prepare_memory(z)\n",
    "            print(memory.shape)\n",
    "            return self.decoder.generate(\n",
    "                memory=memory,\n",
    "                z=z,\n",
    "                label=label,\n",
    "                max_len=self.max_length,\n",
    "                temperature=temperature,\n",
    "                bos_id=self.BOS_ID,\n",
    "                eos_id=self.EOS_ID\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "rSBeiQ44HXNq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Token_Embedding(nn.Module):\n",
    "  def __init__(self,vocab_size,embedding_dim):\n",
    "    super(Token_Embedding,self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.embedding(x)\n",
    "\n",
    "class Pos_Embedding(nn.Module):\n",
    "  def __init__(self,max_len,embedding_dim):\n",
    "    super(Pos_Embedding,self).__init__()\n",
    "    self.pos_embedding = nn.Embedding(max_len,embedding_dim)\n",
    "\n",
    "  def forward(self,x):\n",
    "    seq_len = x.size(1)\n",
    "    pos_ids = torch.arange(seq_len,device=x.device).unsqueeze(0)\n",
    "    return self.pos_embedding(pos_ids)\n",
    "\n",
    "class Transformer_Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, num_layers, ff_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ff_dim,\n",
    "                batch_first=True  # ← Critical for your input shape\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class LatentSpace_Mean_Log(nn.Module):\n",
    "  def __init__(self,embedding_dim,latent_dim):\n",
    "    super(LatentSpace_Mean_Log,self).__init__()\n",
    "    self.fc_mu = nn.Linear(embedding_dim,latent_dim)\n",
    "    self.fc_logvar = nn.Linear(embedding_dim,latent_dim)\n",
    "\n",
    "  def forward(self,x):\n",
    "    mu = self.fc_mu(x)\n",
    "    logvar = self.fc_logvar(x)\n",
    "\n",
    "    return mu,logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Wn_47aTe66-3"
   },
   "outputs": [],
   "source": [
    "class Variational_Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len=502, latent_dim=64,\n",
    "                 num_heads=8, num_layers=6, ff_dim=512, label_dim=34):\n",
    "        super().__init__()\n",
    "        assert max_len >= 502, \"max_len must cover BOS+EOS+500 tokens\"\n",
    "        self.token_embedding = Token_Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = Pos_Embedding(max_len, embedding_dim)\n",
    "        self.encoder = Transformer_Encoder(embedding_dim, num_heads, num_layers, ff_dim)\n",
    "        self.latent_proj = LatentSpace_Mean_Log(embedding_dim, latent_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        # Optional label conditioning\n",
    "        self.label_projection = nn.Embedding(label_dim, embedding_dim) if label_dim > 0 else None\n",
    "        self.device = 'cuda'\n",
    "        # initialization\n",
    "        #token embedding\n",
    "        torch.nn.init.xavier_uniform_(self.token_embedding.embedding.weight)\n",
    "        #posembedding\n",
    "        torch.nn.init.xavier_uniform_(self.pos_embedding.pos_embedding.weight)\n",
    "        #latentspacemeanlog\n",
    "        torch.nn.init.xavier_uniform_(self.latent_proj.fc_mu.weight)\n",
    "        torch.nn.init.zeros_(self.latent_proj.fc_mu.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.latent_proj.fc_logvar.weight)\n",
    "        torch.nn.init.zeros_(self.latent_proj.fc_logvar.bias)\n",
    "        #labelprojection\n",
    "        torch.nn.init.xavier_uniform_(self.label_projection.weight)\n",
    "\n",
    "        for layer in self.encoder.layers:\n",
    "            # Initialize the multi-head attention layers\n",
    "            torch.nn.init.xavier_uniform_(layer.self_attn.in_proj_weight)  # Query, Key, Value weights\n",
    "            torch.nn.init.xavier_uniform_(layer.self_attn.out_proj.weight)  # Output projection\n",
    "    \n",
    "            \n",
    "            # Initialize feed-forward network weights\n",
    "            torch.nn.init.xavier_uniform_(layer.linear1.weight)  # First linear layer of FFN\n",
    "            torch.nn.init.xavier_uniform_(layer.linear2.weight)  # Second linear layer of FFN\n",
    "            \n",
    "            # Initialize layer normalization weights\n",
    "            torch.nn.init.constant_(layer.norm1.weight, 1)  # Norm1 (self-attention layer)\n",
    "            torch.nn.init.constant_(layer.norm2.weight, 1)  # Norm2 (feed-forward layer)\n",
    "            \n",
    "            # Initialize biases\n",
    "            torch.nn.init.zeros_(layer.self_attn.in_proj_bias)  # Bias for attention weights\n",
    "            torch.nn.init.zeros_(layer.self_attn.out_proj.bias)  # Bias for attention output\n",
    "            torch.nn.init.zeros_(layer.linear1.bias)  # Bias for first linear layer\n",
    "            torch.nn.init.zeros_(layer.linear2.bias)  # Bias for second linear layer\n",
    "            torch.nn.init.zeros_(layer.norm1.bias)  # Bias for Norm1\n",
    "            torch.nn.init.zeros_(layer.norm2.bias)  # Bias for Norm2\n",
    "            \n",
    "    def forward(self, x, label=None):\n",
    "        # Input x: [batch_size, 502]\n",
    "        #print(f\"Input shape: {x.shape}\")  # Should be [batch, 502]\n",
    "        #print(f\"Input device: {x.device}\")\n",
    "        tok_emb = self.token_embedding(x)  # [B, 502, D]\n",
    "        pos_emb = self.pos_embedding(x)    # [B, 502, D]\n",
    "        embeddings = tok_emb + pos_emb\n",
    "\n",
    "        # Inject label info (if provided)\n",
    "        if self.label_projection and label is not None:\n",
    "            label_emb = self.label_projection(label.to(self.device)).unsqueeze(1)  # [B, 1, D]\n",
    "            #shape of label_emb = [16,1,128]\n",
    "            #shape of embedding = [16,502,128]\n",
    "            #x.size(1) is 502\n",
    "            temp = label_emb.expand(-1, x.size(1), -1)  # [B, 502, D]\n",
    "            embeddings = embeddings + temp\n",
    "        \n",
    "        # Transformer process\n",
    "        output = self.encoder(embeddings)  # [B, 502, D]\n",
    "\n",
    "        # Pool and project to latent space\n",
    "        pooled = output.mean(dim=1)  # [B, D]\n",
    "        mu, logvar = self.latent_proj(pooled)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "\n",
    "        return z, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MTfiJo80fvcn"
   },
   "outputs": [],
   "source": [
    "#!!! Adapted from HW5!!!\n",
    "\n",
    "from torch import nn, Tensor\n",
    "import torch\n",
    "import math\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Cross-Attention Transformer Decoder\n",
    "class Transformer_Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 d_hid: int,\n",
    "                 nlayers: int,\n",
    "                 latent_dim: int,\n",
    "                 label_dim: int,\n",
    "                 dropout: float = 0.5,\n",
    "                 device = 'cuda'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.device = device\n",
    "        # discrete latent tokens\n",
    "        # self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        # predicted MIDI tokens\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        self.memory_proj = nn.Linear(latent_dim, d_model)\n",
    "\n",
    "        \n",
    "        # Decoder (self-attention and cross-attention)\n",
    "        dec_layer = TransformerDecoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(dec_layer, nlayers)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        #weight modulation layers\n",
    "        self.z_scale = nn.Linear(latent_dim, d_model)\n",
    "        self.z_shift = nn.Linear(latent_dim, d_model)\n",
    "        # label dim should be B x\n",
    "        #label conditioning layers for cross attention\n",
    "        self.label_projection = nn.Embedding(label_dim,d_model)\n",
    "        self.label_attn = nn.MultiheadAttention(d_model,nhead,dropout=dropout)\n",
    "        # Project z to scaling factors\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.tgt_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        torch.nn.init.xavier_uniform_(self.tgt_embedding.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.label_projection.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.memory_proj.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.z_scale.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.z_shift.weight)\n",
    "        self.memory_proj.bias.data.zero_()\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.z_scale.bias.data.zero_()\n",
    "        self.z_shift.bias.data.zero_()\n",
    "        for layer in self.transformer_decoder.layers:\n",
    "            # Initialize multi-head attention weights\n",
    "            torch.nn.init.xavier_uniform_(layer.self_attn.in_proj_weight)\n",
    "            torch.nn.init.xavier_uniform_(layer.self_attn.out_proj.weight)\n",
    "            torch.nn.init.xavier_uniform_(layer.multihead_attn.in_proj_weight)\n",
    "            torch.nn.init.xavier_uniform_(layer.multihead_attn.out_proj.weight)\n",
    "        \n",
    "        # Initialize feed-forward network weights\n",
    "        torch.nn.init.xavier_uniform_(layer.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(layer.linear2.weight)\n",
    "        \n",
    "        # Initialize layer norms\n",
    "        torch.nn.init.constant_(layer.norm1.weight, 1)\n",
    "        torch.nn.init.constant_(layer.norm2.weight, 1)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.label_attn.in_proj_weight)\n",
    "        torch.nn.init.xavier_uniform_(self.label_attn.out_proj.weight)\n",
    "\n",
    "    def forward(self, tgt: Tensor,memory: Tensor,z : Tensor, label:Tensor,teacher_forcing=True):\n",
    "        # Scale and Embed the memory sequence\n",
    "        memory = self.memory_proj(memory) * math.sqrt(self.d_model)\n",
    "        # Add positional encoding\n",
    "        memory = self.pos_encoder(memory)\n",
    "        # teacher forcing so that during training we can train well but also\n",
    "        # autoregressively predict during evaluation\n",
    "        #print(f'tgt shape {tgt.shape}')\n",
    "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        #print(f'tgt emb{tgt_emb.shape}')\n",
    "        shift = self.z_shift(z)\n",
    "        scale = self.z_scale(z)\n",
    "        #print(f'shift{shift.shape}')\n",
    "        #print(f'scale{scale.shape}')\n",
    "        #print(f'tgt shape{tgt_emb.shape} shift shape {shift.shape} scale shape {scale.shape}')\n",
    "        tgt_emb = tgt_emb * scale.unsqueeze(0) + shift.unsqueeze(0)\n",
    "    \n",
    "        label_emb = self.label_projection(label).unsqueeze(0)\n",
    "        label_emb = label_emb.expand(tgt_emb.shape[0], -1, -1)  # (batch_size,seq_len, d_model)\n",
    "        #print(f'tgt emb{tgt_emb.shape} | label_emb shape {label_emb.shape}')\n",
    "        tgt_emb = self.label_attn(query=tgt_emb, key=label_emb, value=label_emb)[0]\n",
    "            \n",
    "        # Create the mask\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(len(tgt_emb)).to(self.device)\n",
    "        # Pass them through the transformer\n",
    "        output = self.transformer_decoder(tgt=tgt_emb, memory=memory, tgt_mask=tgt_mask)\n",
    "        # Apply the linear layer\n",
    "        output = self.linear(output)\n",
    "        output = output.transpose(0, 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def generate(self, memory, z, label, max_len=502, temperature=1.0, bos_id=129, eos_id=130):\n",
    "        batch_size = z.size(0)\n",
    "        device = self.device\n",
    "    \n",
    "        # 1. Start with [BOS] token for all sequences in batch\n",
    "        tgt = torch.full((1, batch_size), bos_id, device=device)  # [1, batch_size]\n",
    "        # 2. Project and encode memory\n",
    "        memory = self.memory_proj(memory) * math.sqrt(self.d_model)\n",
    "        memory = self.pos_encoder(memory)\n",
    "\n",
    "        #logits_list = []\n",
    "        for i in range(max_len-1):\n",
    "            # 3. Forward pass\n",
    "            logits = self(\n",
    "                tgt=tgt,  # Transformer expects [seq_len, batch_size]\n",
    "                memory=memory,\n",
    "                z=z,\n",
    "                label=label,\n",
    "                teacher_forcing=False\n",
    "            ) # Get logits for last token only [batch_size, vocab_size]\n",
    "            #logits_list.append(logits)\n",
    "            logits = logits.transpose(0, 1)[-1,:,:]\n",
    "            # 4. Sample next token\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples= 1).transpose(0, 1)  # [1, batch_size]\n",
    "            # 5. Append next token\n",
    "            tgt = torch.cat([tgt, next_token], dim=0)  # along seq_len (dim=0)\n",
    "            if i == max_len - 1:\n",
    "                next_token.fill_(eos_id)  # Force EOS token at max_len\n",
    "                print(\"Forcing <EOS> token at 502nd position.\")\n",
    "                tgt = torch.cat([tgt, next_token], dim=0)\n",
    "                break\n",
    "            # 6. Early stopping if all samples predict EOS\n",
    "            if (next_token == eos_id).all():\n",
    "                break\n",
    "        #print(f'tgt shape{tgt.shape}')\n",
    "        return tgt#,torch.cat(logits_list, dim=0)  # [seq_len-1, batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vqa0QC59r17",
    "outputId": "605f2716-c6d8-4cf5-c748-f4897f878977"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6wuW-wFzEBts"
   },
   "outputs": [],
   "source": [
    "file_directory = '/content/drive/MyDrive/maestro_token_sequences.csv' # colab\n",
    "file_directory = '/projectnb/ec523/projects/proj_MIDIgen/maestro_new_splits_no_augmentation.csv' #scc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "def string_to_vector(s):\n",
    "    numbers = [int(x) for x in s.split()]\n",
    "    return numbers\n",
    "\n",
    "\n",
    "def prepare_data(file, augmentations=False, collaborations=False,split='train'):\n",
    "    # Read the data\n",
    "    df = pd.read_csv(file)\n",
    "    print(df.head())\n",
    "    # only read train data\n",
    "    if(split != 'all'):\n",
    "      df = df[df['split'] == split].copy()\n",
    "\n",
    "    # Convert sequences to vectors\n",
    "    df['sequence_vector'] = df['sequence_vector'].apply(string_to_vector)\n",
    "    # Apply filters\n",
    "    if (not collaborations):\n",
    "        df = df[~df['label'].str.contains('/', na=False)].copy()\n",
    "\n",
    "    if (not augmentations):\n",
    "        df = df[df['transposition amount'] == 0].copy()\n",
    "\n",
    "    # Create labels (assuming composer_label_dict exists)\n",
    "    clean_df = df.copy()  # Final cleaned version\n",
    "    composer_list = sorted(list(set(clean_df['label'])))  # Convert to sorted list for consistent ordering\n",
    "    num_composers = len(composer_list)\n",
    "    print(f\"Unique composers: {composer_list}\")\n",
    "    print(f\"Total composers: {num_composers}\")\n",
    "\n",
    "    # Create proper label dictionary\n",
    "    composer_label_dict = {composer: idx for idx, composer in enumerate(composer_list)}\n",
    "    index_composer_dict = {idx: composer for composer, idx in composer_label_dict.items()}\n",
    "    clean_df['label'] = clean_df['label'].map(composer_label_dict)\n",
    "\n",
    "    # Select only the two columns we want\n",
    "    data = clean_df[['sequence_vector', 'label']].copy()\n",
    "\n",
    "    return data,composer_label_dict,index_composer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9EDS3PnCFnQg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "def string_to_vector(s):\n",
    "    s = s.strip('[]')  # Removes [ and ] from the start and end\n",
    "    numbers = [int(x.strip()) for x in s.split(',')]\n",
    "    return numbers\n",
    "\n",
    "\n",
    "def prepare_data(file, augmentations=False, collaborations=False,split='train'):\n",
    "    # Read the data\n",
    "    df = pd.read_csv(file)\n",
    "    print(df.head())\n",
    "    # only read train data\n",
    "    if(split != 'all'):\n",
    "      df = df[df['split'] == split].copy()\n",
    "\n",
    "    # Convert sequences to vectors\n",
    "    df['sequence_vector'] = df['sequence_vector'].apply(string_to_vector)\n",
    "    # Apply filters\n",
    "\n",
    "    # Create labels (assuming composer_label_dict exists)\n",
    "    clean_df = df.copy()  # Final cleaned version\n",
    "    composer_list = sorted(list(set(clean_df['label'])))  # Convert to sorted list for consistent ordering\n",
    "    num_composers = len(composer_list)\n",
    "    print(f\"Unique composers: {composer_list}\")\n",
    "    print(f\"Total composers: {num_composers}\")\n",
    "\n",
    "    # Create proper label dictionary\n",
    "    composer_label_dict = {composer: idx for idx, composer in enumerate(composer_list)}\n",
    "    index_composer_dict = {idx: composer for composer, idx in composer_label_dict.items()}\n",
    "    clean_df['label'] = clean_df['label'].map(composer_label_dict)\n",
    "\n",
    "    # Select only the two columns we want\n",
    "    data = clean_df[['sequence_vector', 'label']].copy()\n",
    "\n",
    "    return data,composer_label_dict,index_composer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FMCJfN0PiOuD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "class MIDI_Dataset(Dataset):\n",
    "  def __init__(self,sequences,labels):\n",
    "    self.sequences = sequences\n",
    "    self.labels = labels\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.sequences)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "      # Convert to tensor directly (no padding needed)\n",
    "      seq = torch.tensor(self.sequences[idx], dtype=torch.long)\n",
    "\n",
    "      if self.labels is not None:\n",
    "          label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "          return seq, label\n",
    "      return seq\n",
    "def create_MIDI_Dataloaders(train_data, batch_size=32):\n",
    "\n",
    "  # Create datasets\n",
    "  train_dataset = MIDI_Dataset(\n",
    "      sequences=train_data['sequence_vector'].tolist(),\n",
    "      labels=train_data['label']\n",
    "  )\n",
    "\n",
    "  # Create dataloaders\n",
    "  train_loader = DataLoader(\n",
    "      train_dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      pin_memory=True,\n",
    "      num_workers=4  # Parallel loading\n",
    "  )\n",
    "\n",
    "  return train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     sequence_vector  label  split\n",
      "0  [129, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, ...     20  train\n",
      "1  [129, 64, 64, 64, 67, 60, 60, 62, 62, 62, 60, ...      9  train\n",
      "2  [129, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, ...     23  train\n",
      "3  [129, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, ...      2  train\n",
      "4  [129, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, ...     32  train\n",
      "Unique composers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
      "Total composers: 34\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "data,composer_to_label_map,inv_map = prepare_data(file_directory,augmentations=False,split='train')\n",
    "train_data = create_MIDI_Dataloaders(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Tried 256 hidden layers initiallty, but overfitted... :( (Also increased dropout from 0.3 to 0.5)\n",
    "class MidiGRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=131, embed_dim=128, hidden_size=128, num_layers=2, num_classes=34, bidirectional=True, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1  \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, num_layers,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout if num_layers > 1 else 0,\n",
    "                          bidirectional=bidirectional)\n",
    "        self.norm = nn.LayerNorm(hidden_size * self.num_directions)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.num_directions, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.norm(out[:, -1, :])\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "sdooHH7WGrsY",
    "outputId": "2b6e43ef-bbd5-4325-f166-3d991084de3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25192, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/4636900.1.ece/ipykernel_2282928/3130511945.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier = torch.load(\"midi_gru_classifier_v4.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder initialized with:\n",
      "- Vocab size: 131\n",
      "- Embedding dim: 128\n",
      "- Latent dim: 128\n",
      "- 6 layers with 8 attention heads each\n",
      "\n",
      "Decoder initialized with:\n",
      "- Same vocab size: 131\n",
      "- Label embedding dim: 34\n",
      "- Hidden dim: 256\n",
      "- Using dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.memory._record_memory_history()\n",
    "\n",
    "# Shared Parameters\n",
    "vocab_size = 131        # Number of unique MIDI tokens\n",
    "embedding_dim = 128     # Size of token embeddings\n",
    "max_len = 502           # Max sequence length (500 tokens + EOS + BOS)\n",
    "latent_dim = 128        # Latent space dimension\n",
    "num_heads = 8           # Number of attention heads\n",
    "num_layers = 6          # Number of transformer layers\n",
    "ff_dim = 512            # Feed-forward layer dimension\n",
    "dropout = 0.1           # Dropout rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Additional Decoder-specific Parameters\n",
    "label_dim = 34          # Dimension for label embeddings (# of composers)\n",
    "hidden_dim = 256        # Hidden dimension in decoder (d_hid in your code)\n",
    "\n",
    "print(data.shape)\n",
    "# Initialize Encoder\n",
    "encoder = Variational_Encoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    max_len=max_len,\n",
    "    latent_dim=latent_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    ff_dim=ff_dim\n",
    ").to(device)\n",
    "\n",
    "# Initialize Decoder\n",
    "decoder = Transformer_Decoder(\n",
    "    src_vocab_size=vocab_size,    # Same as encoder vocab size\n",
    "    tgt_vocab_size=vocab_size,    # Same unless you have different input/output vocabs\n",
    "    d_model=embedding_dim,        # Should match encoder's embedding_dim\n",
    "    nhead=num_heads,              # Same as encoder\n",
    "    d_hid=hidden_dim,             # Decoder-specific hidden dim\n",
    "    nlayers=num_layers,           # Same as encoder\n",
    "    latent_dim=latent_dim,        # Same as encoder\n",
    "    label_dim=label_dim,          # For composer/style conditioning\n",
    "    device=device,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "#load classifier\n",
    "#vocab_size=131, embed_dim=128, hidden_size=128, num_layers=2, num_classes=len(label_map)\n",
    "classifier = torch.load(\"midi_gru_classifier_v4.pth\", map_location=device)\n",
    "\n",
    "classifier.eval()\n",
    "classifier.to(device)\n",
    "\n",
    "# workout since classifier was trained with \n",
    "model = MIDIVAE(encoder,decoder,classifier,max_len)\n",
    "optimizer = optim.Adam([{'params': encoder.parameters(), 'lr': 3e-4},{'params': decoder.parameters(), 'lr': 3e-4},{'params':model.memory_proj.parameters(),'lr':3e-4}],weight_decay=1e-5) \n",
    "                       \n",
    "\n",
    "\n",
    "print(\"Encoder initialized with:\")\n",
    "print(f\"- Vocab size: {vocab_size}\")\n",
    "print(f\"- Embedding dim: {embedding_dim}\")\n",
    "print(f\"- Latent dim: {latent_dim}\")\n",
    "print(f\"- {num_layers} layers with {num_heads} attention heads each\")\n",
    "\n",
    "print(\"\\nDecoder initialized with:\")\n",
    "print(f\"- Same vocab size: {vocab_size}\")\n",
    "print(f\"- Label embedding dim: {label_dim}\")\n",
    "print(f\"- Hidden dim: {hidden_dim}\")\n",
    "print(f\"- Using dropout: {dropout}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 788/788 [01:42<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 5.9371 | KLD: 4.1826075403150753e-07 | CLS: 0.002972245682030916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 788/788 [01:42<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 5.8665 | KLD: 3.519113249694783e-07 | CLS: 0.001901553594507277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 788/788 [01:42<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 6.7193 | KLD: 3.450830661222426e-07 | CLS: 0.0024236852768808603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 788/788 [01:43<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 5.8405 | KLD: 1.8499949305805785e-07 | CLS: 0.0021625205408781767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 788/788 [01:43<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 6.2528 | KLD: 2.112688690658615e-07 | CLS: 0.0021148943342268467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 788/788 [01:43<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss: 5.8849 | KLD: 1.58731623400854e-07 | CLS: 0.0023847969714552164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 788/788 [01:44<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss: 6.0690 | KLD: 1.1856084114469922e-07 | CLS: 0.0017963640857487917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 788/788 [01:45<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss: 5.9767 | KLD: 1.0025456731455051e-07 | CLS: 0.0021744626574218273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 788/788 [01:44<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss: 6.0164 | KLD: 9.072479656424548e-08 | CLS: 0.002223146380856633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 788/788 [01:44<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss: 5.9900 | KLD: 7.461472506520295e-08 | CLS: 0.0031632618047297\n"
     ]
    }
   ],
   "source": [
    "#def train_model(self, dataloader, optimizer, epochs=10):\n",
    "model.train_model(train_data,optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/projectnb/ec523/projects/proj_MIDIgen/MIDIVAE_v2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/4636900.1.ece/ipykernel_2282928/1861264424.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('MIDIVAE_v2.pth',map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "# Generate Songs Testing\n",
    "model = MIDIVAE(encoder,decoder,classifier,max_len)\n",
    "model.load_state_dict(torch.load('MIDIVAE_v2.pth',map_location=device))\n",
    "z = torch.randn(1, 128).to('cuda') \n",
    "label = torch.LongTensor([12]).to('cuda')\n",
    "generated_seq = model.generate(z,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mido in /usr4/ece601/melnick/.local/lib/python3.11/site-packages (1.3.3)\n",
      "Requirement already satisfied: packaging in /share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages (from mido) (24.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pretty_midi in /usr4/ece601/melnick/.local/lib/python3.11/site-packages (0.2.10)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages (from pretty_midi) (1.26.4)\n",
      "Requirement already satisfied: mido>=1.1.16 in /usr4/ece601/melnick/.local/lib/python3.11/site-packages (from pretty_midi) (1.3.3)\n",
      "Requirement already satisfied: six in /share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages (from pretty_midi) (1.17.0)\n",
      "Requirement already satisfied: packaging in /share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages (from mido>=1.1.16->pretty_midi) (24.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install mido\n",
    "! pip install pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to generated_sequence_pretty.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/4636900.1.ece/ipykernel_2282928/559416346.py:15: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  token = int(token)        # make doubly sure it’s a Python int\n"
     ]
    }
   ],
   "source": [
    "import pretty_midi\n",
    "\n",
    "# Your tensor\n",
    "tokens = generated_seq.cpu().numpy()\n",
    "\n",
    "# Create a PrettyMIDI object\n",
    "midi = pretty_midi.PrettyMIDI()\n",
    "instrument = pretty_midi.Instrument(program=0)  # Acoustic Grand Piano\n",
    "\n",
    "current_time = 0.0\n",
    "note_duration = 0.5  # Seconds per note\n",
    "rest_duration = 0.5\n",
    "\n",
    "for token in tokens:\n",
    "    token = int(token)        # make doubly sure it’s a Python int\n",
    "    if token == 129:\n",
    "        continue  # BOS\n",
    "    if token == 130:\n",
    "        break  # EOS\n",
    "    if token == 128:\n",
    "        # Rest: just advance time\n",
    "        current_time += rest_duration\n",
    "    elif 0 <= token <= 127:\n",
    "        # Create a Note\n",
    "        note = pretty_midi.Note(\n",
    "            velocity=100,  # fixed velocity\n",
    "            pitch=token,\n",
    "            start=current_time,\n",
    "            end=current_time + note_duration\n",
    "        )\n",
    "        instrument.notes.append(note)\n",
    "        current_time += note_duration  # move forward\n",
    "\n",
    "# Add instrument to MIDI and write it out\n",
    "midi.instruments.append(instrument)\n",
    "output_midi = \"generated_sequence_pretty.mid\"\n",
    "midi.write(output_midi)\n",
    "\n",
    "print(f\"Saved to {output_midi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_seq.transpose(0,1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_seq2.transpose(0,1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(generated_seq3.transpose(0,1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Space Analysis\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
