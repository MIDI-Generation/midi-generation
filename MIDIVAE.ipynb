{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "mdZlKX0m42z3"
   },
   "outputs": [],
   "source": [
    "! CUDA_LAUNCH_BLOCKING=1\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "REST_ID = 128\n",
    "BOS_ID = 129\n",
    "EOS_ID = 130\n",
    "\n",
    "class MIDIVAE(nn.Module):\n",
    "  def __init__(self, encoder, decoder,classifier,max_length=502,device = 'cuda',kld_weight = 1.0, classifier_weight = 1.0):\n",
    "      super(MIDIVAE,self).__init__()\n",
    "\n",
    "      self.device = device\n",
    "      # untrained components\n",
    "      self.encoder = encoder.to(self.device)\n",
    "      self.decoder = decoder.to(self.device)  # Cross-Attention transformer to generate MIDI vectors\n",
    "      self.max_length = max_length\n",
    "      #workaround for cross-attention\n",
    "      self.num_memory_tokens = (encoder.latent_dim // 16)\n",
    "      self.memory_proj = nn.Sequential(\n",
    "            nn.Linear(encoder.latent_dim, decoder.d_model * self.num_memory_tokens),\n",
    "            nn.Unflatten(1, (self.num_memory_tokens, decoder.d_model))  # [B, num_tokens, d_model]\n",
    "        ).to(self.device)\n",
    "      # pre-trained classifier\n",
    "      self.classifier = classifier\n",
    "\n",
    "      #constants\n",
    "      self.kld_weight = kld_weight\n",
    "      self.classifier_weight = classifier_weight\n",
    "      self.BOS_ID = BOS_ID\n",
    "      self.EOS_ID = EOS_ID\n",
    "      self.REST_ID = REST_ID\n",
    "\n",
    "  def forward(self, x, label, teacher_forcing_ratio=0.5):\n",
    "        z, mean, logvar = self.encoder(x, label)\n",
    "\n",
    "        memory = self._prepare_memory(z.to(self.device)).to(self.device)\n",
    "\n",
    "        tgt = torch.full((1, x.shape[0]), self.BOS_ID, device=self.device)  # [1, batch_size]\n",
    "        generated_tokens = tgt\n",
    "        #print(f'generated tok shape { generated_tokens.shape}')\n",
    "        # logits_list = []\n",
    "      \n",
    "        for i in range(501): \n",
    "            if self.training and random.random() < teacher_forcing_ratio:\n",
    "                # Pass the ground truth (x) as the target for the next token\n",
    "                #print('------TEACHER FORCING--------')\n",
    "                #print(x.shape)\n",
    "                #print(x[:,:i+1].shape)\n",
    "                next_token_logits = self.decoder(\n",
    "                    tgt = x[:,:i+1].transpose(0,1),  # Use ground truth target token sequence\n",
    "                    memory=memory,  # Latent memory\n",
    "                    z=z,  # Latent representation from encoder\n",
    "                    label=label,  # Conditioning label\n",
    "                    teacher_forcing=True  # Enable teacher forcing\n",
    "                ).transpose(0, 1)\n",
    "            else:\n",
    "                #print('------ FREESTYLE--------')\n",
    "                #print(f'generated tok shape { generated_tokens.shape}')\n",
    "                next_token_logits = self.decoder(\n",
    "                    tgt=generated_tokens,  # Use generated tokens\n",
    "                    memory=memory,  # Latent memory\n",
    "                    z=z,  # Latent representation from encoder\n",
    "                    label=label,  # Conditioning label\n",
    "                    teacher_forcing=False  # Disable teacher forcing\n",
    "                ).transpose(0, 1)  # Adjust shape for further processing\n",
    "            #logits_list.append(next_token_logits)\n",
    "            next_token_logits = next_token_logits[-1, :, :]  # [1, batch_size, vocab_size]\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # [1, batch_size]\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=0)\n",
    "            # Force EOS token at max length (502 tokens)\n",
    "            if i == 501:\n",
    "                next_token.fill_(self.EOS_ID)  # Force EOS token at max_len\n",
    "                print(\"Forcing <EOS> token at 502nd position.\")\n",
    "                generated_tokens = torch.cat([generated_tokens, next_token], dim=0)\n",
    "                break\n",
    "            if (next_token == self.EOS_ID).all():\n",
    "                print(\"Forcing <EOS> token at max length.\")\n",
    "                break\n",
    "            #print(f'generate seq shape {generated_tokens.shape},next_token shape{next_token.shape}')\n",
    "        recon_midi = generated_tokens  # Final generated sequence of tokens\n",
    "        composer_pred = self.classifier(recon_midi.transpose(0,1))\n",
    "\n",
    "        return recon_midi,mean, logvar, composer_pred\n",
    "\n",
    "  def train_model(self, dataloader, optimizer, epochs=10):\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x, label in tqdm(dataloader,desc = f\"Training Epoch {epoch + 1}\"):\n",
    "                x, label = x.to(self.device), label.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                recon_x, mu, logvar, pred = self(x, label)\n",
    "                #recon_x, token_logits,mu, logvar = self(x, label)\n",
    "                \n",
    "                # Loss calculations\n",
    "                kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                #print(f'x data type {x.dtype} recon data type {recon_x.dtype}')\n",
    "                #print(f'recon shape{recon_x.transpose(0,1).shape},x shape {x.shape}')\n",
    "                recon_loss = F.mse_loss(\n",
    "                    recon_x.transpose(0,1).to(torch.float32),  # [batch,seq_len]\n",
    "                    x.to(torch.float32)  # [batch, seq_len]\n",
    "                )\n",
    "                \n",
    "                cls_loss = F.cross_entropy(pred, label)\n",
    "\n",
    "                loss = recon_loss + self.kld_weight*kld + self.classifier_weight*cls_loss\n",
    "                #loss = recon_loss + self.kld_weight*kld\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "  def _prepare_memory(self, z):\n",
    "        memory = self.memory_proj(z).to(self.device)  # [B, num_tokens, d_model]\n",
    "        return memory.transpose(0, 1).to(self.device)\n",
    "\n",
    "  def generate(self, z, label, temperature=1.0):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            memory = self._prepare_memory(z)\n",
    "            return self.decoder.generate(\n",
    "                memory=memory,\n",
    "                z=z,\n",
    "                label=label,\n",
    "                max_len=self.max_length,\n",
    "                temperature=temperature,\n",
    "                bos_id=self.bos_id,\n",
    "                eos_id=self.eos_id\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "rSBeiQ44HXNq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Token_Embedding(nn.Module):\n",
    "  def __init__(self,vocab_size,embedding_dim):\n",
    "    super(Token_Embedding,self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.embedding(x)\n",
    "\n",
    "class Pos_Embedding(nn.Module):\n",
    "  def __init__(self,max_len,embedding_dim):\n",
    "    super(Pos_Embedding,self).__init__()\n",
    "    self.pos_embedding = nn.Embedding(max_len,embedding_dim)\n",
    "\n",
    "  def forward(self,x):\n",
    "    seq_len = x.size(1)\n",
    "    pos_ids = torch.arange(seq_len,device=x.device).unsqueeze(0)\n",
    "    return self.pos_embedding(pos_ids)\n",
    "\n",
    "class Transformer_Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, num_layers, ff_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ff_dim,\n",
    "                batch_first=True  # ← Critical for your input shape\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class LatentSpace_Mean_Log(nn.Module):\n",
    "  def __init__(self,embedding_dim,latent_dim):\n",
    "    super(LatentSpace_Mean_Log,self).__init__()\n",
    "    self.fc_mu = nn.Linear(embedding_dim,latent_dim)\n",
    "    self.fc_logvar = nn.Linear(embedding_dim,latent_dim)\n",
    "\n",
    "  def forward(self,x):\n",
    "    mu = self.fc_mu(x)\n",
    "    logvar = self.fc_logvar(x)\n",
    "\n",
    "    return mu,logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "Wn_47aTe66-3"
   },
   "outputs": [],
   "source": [
    "class Variational_Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len=502, latent_dim=64,\n",
    "                 num_heads=8, num_layers=6, ff_dim=512, label_dim=0):\n",
    "        super().__init__()\n",
    "        assert max_len >= 502, \"max_len must cover BOS+EOS+500 tokens\"\n",
    "        self.token_embedding = Token_Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = Pos_Embedding(max_len, embedding_dim)\n",
    "        self.encoder = Transformer_Encoder(embedding_dim, num_heads, num_layers, ff_dim)\n",
    "        self.latent_proj = LatentSpace_Mean_Log(embedding_dim, latent_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        # Optional label conditioning\n",
    "        self.label_projection = nn.Linear(label_dim, embedding_dim) if label_dim > 0 else None\n",
    "\n",
    "    def forward(self, x, label=None):\n",
    "        # Input x: [batch_size, 502]\n",
    "        #print(f\"Input shape: {x.shape}\")  # Should be [batch, 502]\n",
    "        #print(f\"Input device: {x.device}\")\n",
    "        tok_emb = self.token_embedding(x)  # [B, 502, D]\n",
    "        pos_emb = self.pos_embedding(x)    # [B, 502, D]\n",
    "        embeddings = tok_emb + pos_emb\n",
    "\n",
    "        # Inject label info (if provided)\n",
    "        if self.label_projection and label is not None:\n",
    "            label_emb = self.label_projection(label).unsqueeze(1)  # [B, 1, D]\n",
    "            embeddings += label_emb.expand(-1, x.size(1), -1)  # [B, 502, D]\n",
    "\n",
    "        # Transformer process\n",
    "        output = self.encoder(embeddings)  # [B, 502, D]\n",
    "\n",
    "        # Pool and project to latent space\n",
    "        pooled = output.mean(dim=1)  # [B, D]\n",
    "        mu, logvar = self.latent_proj(pooled)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "\n",
    "        return z, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "MTfiJo80fvcn"
   },
   "outputs": [],
   "source": [
    "#!!! Adapted from HW5!!!\n",
    "\n",
    "from torch import nn, Tensor\n",
    "import torch\n",
    "import math\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Cross-Attention Transformer Decoder\n",
    "class Transformer_Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 d_hid: int,\n",
    "                 nlayers: int,\n",
    "                 latent_dim: int,\n",
    "                 label_dim: int,\n",
    "                 dropout: float = 0.5,\n",
    "                 device = 'cuda'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.device = device\n",
    "        # discrete latent tokens\n",
    "        # self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        # predicted MIDI tokens\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        self.memory_proj = nn.Linear(latent_dim, d_model)\n",
    "\n",
    "        \n",
    "        # Decoder (self-attention and cross-attention)\n",
    "        dec_layer = TransformerDecoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(dec_layer, nlayers)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        #weight modulation layers\n",
    "        self.z_scale = nn.Linear(latent_dim, d_model)\n",
    "        self.z_shift = nn.Linear(latent_dim, d_model)\n",
    "        # label dim should be B x\n",
    "        #label conditioning layers for cross attention\n",
    "        self.label_projection = nn.Embedding(label_dim,d_model)\n",
    "        self.label_attn = nn.MultiheadAttention(d_model,nhead,dropout=dropout)\n",
    "        # Project z to scaling factors\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.memory_proj.weight.data.uniform_(-initrange, initrange)\n",
    "        self.tgt_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, tgt: Tensor,memory: Tensor,z : Tensor, label:Tensor,teacher_forcing=True):\n",
    "        # Scale and Embed the memory sequence\n",
    "        #print(f'Memory Shape{memory.shape}')\n",
    "        memory = self.memory_proj(memory) * math.sqrt(self.d_model)\n",
    "        # Add positional encoding\n",
    "        memory = self.pos_encoder(memory)\n",
    "        \n",
    "        # teacher forcing so that during training we can train well but also\n",
    "        # autoregressively predict during evaluation\n",
    "        #print(f'tgt shape {tgt.shape}')\n",
    "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        #print(f'tgt emb{tgt_emb.shape}')\n",
    "        shift = self.z_shift(z)\n",
    "        scale = self.z_scale(z)\n",
    "        #print(f'shift{shift.shape}')\n",
    "        #print(f'scale{scale.shape}')\n",
    "        #print(f'tgt shape{tgt_emb.shape} shift shape {shift.shape} scale shape {scale.shape}')\n",
    "        tgt_emb = tgt_emb * scale.unsqueeze(0) + shift.unsqueeze(0)\n",
    "    \n",
    "        label_emb = self.label_projection(label).unsqueeze(0)\n",
    "        label_emb = label_emb.expand(tgt_emb.shape[0], -1, -1)  # (batch_size,seq_len, d_model)\n",
    "        tgt_emb = self.label_attn(query=tgt_emb, key=label_emb, value=label_emb)[0]\n",
    "            \n",
    "        # Create the mask\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(len(tgt_emb)).to(self.device)\n",
    "        # Pass them through the transformer\n",
    "        output = self.transformer_decoder(tgt=tgt_emb, memory=memory, tgt_mask=tgt_mask)\n",
    "        # Apply the linear layer\n",
    "        output = self.linear(output)\n",
    "        output = output.transpose(0, 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def generate(self, memory, z, label, max_len=502, temperature=1.0, bos_id=129, eos_id=130):\n",
    "        batch_size = z.size(0)\n",
    "        device = self.device\n",
    "    \n",
    "        # 1. Start with [BOS] token for all sequences in batch\n",
    "        tgt = torch.full((1, batch_size), bos_id, device=device)  # [1, batch_size]\n",
    "        # 2. Project and encode memory\n",
    "        memory = self.memory_proj(memory) * math.sqrt(self.d_model)\n",
    "        memory = self.pos_encoder(memory)\n",
    "\n",
    "        logits_list = []\n",
    "        for i in range(max_len-1):\n",
    "            # 3. Forward pass\n",
    "            logits = self(\n",
    "                tgt=tgt.transpose(0, 1),  # Transformer expects [seq_len, batch_size]\n",
    "                memory=memory,\n",
    "                z=z,\n",
    "                label=label,\n",
    "                teacher_forcing=False\n",
    "            ) # Get logits for last token only [batch_size, vocab_size]\n",
    "            logits_list.append(logits)\n",
    "            logits = logits.transpose(0, 1)[-1,:,:]\n",
    "            # 4. Sample next token\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples= 1).transpose(0, 1)  # [1, batch_size]\n",
    "            # 5. Append next token\n",
    "            tgt = torch.cat([tgt, next_token], dim=0)  # along seq_len (dim=0)\n",
    "            if i == max_len - 1:\n",
    "                next_token.fill_(eos_id)  # Force EOS token at max_len\n",
    "                print(\"Forcing <EOS> token at 502nd position.\")\n",
    "                tgt = torch.cat([tgt, next_token], dim=0)\n",
    "                break\n",
    "            # 6. Early stopping if all samples predict EOS\n",
    "            if (next_token == eos_id).all():\n",
    "                break\n",
    "        #print(f'tgt shape{tgt.shape}')\n",
    "        return tgt,torch.cat(logits_list, dim=0)  # [seq_len-1, batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vqa0QC59r17",
    "outputId": "605f2716-c6d8-4cf5-c748-f4897f878977"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6wuW-wFzEBts"
   },
   "outputs": [],
   "source": [
    "file_directory = '/content/drive/MyDrive/maestro_token_sequences.csv' # colab\n",
    "file_directory = '/projectnb/ec523/projects/proj_MIDIgen/maestro_token_sequences.csv' #scc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9EDS3PnCFnQg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "def string_to_vector(seq_str):\n",
    "    return [int(x) for x in seq_str.split()]\n",
    "\n",
    "def prepare_data(file, augmentations=False, collaborations=False,split='train'):\n",
    "    # Read the data\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # only read train data\n",
    "    if(split != 'all'):\n",
    "      df = df[df['split'] == split].copy()\n",
    "\n",
    "    # Convert sequences to vectors\n",
    "    df['sequence_vector'] = df['sequence'].apply(string_to_vector)\n",
    "\n",
    "    # Apply filters\n",
    "    if (not collaborations):\n",
    "        df = df[~df['composer'].str.contains('/', na=False)].copy()\n",
    "\n",
    "    if (not augmentations):\n",
    "        df = df[df['transposition amount'] == 0].copy()\n",
    "\n",
    "    # Create labels (assuming composer_label_dict exists)\n",
    "    clean_df = df.copy()  # Final cleaned version\n",
    "    composer_list = sorted(list(set(clean_df['composer'])))  # Convert to sorted list for consistent ordering\n",
    "    num_composers = len(composer_list)\n",
    "    print(f\"Unique composers: {composer_list}\")\n",
    "    print(f\"Total composers: {num_composers}\")\n",
    "\n",
    "    # Create proper label dictionary\n",
    "    composer_label_dict = {composer: idx for idx, composer in enumerate(composer_list)}\n",
    "    index_composer_dict = {idx: composer for composer, idx in composer_label_dict.items()}\n",
    "    clean_df['label'] = clean_df['composer'].map(composer_label_dict)\n",
    "\n",
    "    # Select only the two columns we want\n",
    "    data = clean_df[['sequence_vector', 'label']].copy()\n",
    "\n",
    "    return data,composer_label_dict,index_composer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "FMCJfN0PiOuD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "class MIDI_Dataset(Dataset):\n",
    "  def __init__(self,sequences,labels):\n",
    "    self.sequences = sequences\n",
    "    self.labels = labels\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.sequences)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "      # Convert to tensor directly (no padding needed)\n",
    "      seq = torch.tensor(self.sequences[idx], dtype=torch.long)\n",
    "\n",
    "      if self.labels is not None:\n",
    "          label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "          return seq, label\n",
    "      return seq\n",
    "def create_MIDI_Dataloaders(train_data, batch_size=16):\n",
    "\n",
    "  # Create datasets\n",
    "  train_dataset = MIDI_Dataset(\n",
    "      sequences=train_data['sequence_vector'].tolist(),\n",
    "      labels=train_data['label']\n",
    "  )\n",
    "\n",
    "  # Create dataloaders\n",
    "  train_loader = DataLoader(\n",
    "      train_dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      pin_memory=True,\n",
    "      num_workers=4  # Parallel loading\n",
    "  )\n",
    "\n",
    "  return train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique composers: ['Alexander Scriabin', 'César Franck', 'Felix Mendelssohn', 'Franz Liszt', 'Franz Schubert', 'Frédéric Chopin', 'Johann Sebastian Bach', 'Johannes Brahms', 'Joseph Haydn', 'Ludwig van Beethoven', 'Mily Balakirev', 'Robert Schumann', 'Sergei Rachmaninoff', 'Wolfgang Amadeus Mozart']\n",
      "Total composers: 14\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "data,composer_to_label_map,inv_map = prepare_data(file_directory,augmentations=False,split='validation')\n",
    "train_data = create_MIDI_Dataloaders(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Tried 256 hidden layers initiallty, but overfitted... :( (Also increased dropout from 0.3 to 0.5)\n",
    "class MidiGRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=131, embed_dim=128, hidden_size=128, num_layers=2, num_classes=34, bidirectional=True, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1  \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, num_layers,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout if num_layers > 1 else 0,\n",
    "                          bidirectional=bidirectional)\n",
    "        self.norm = nn.LayerNorm(hidden_size * self.num_directions)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.num_directions, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.norm(out[:, -1, :])\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "sdooHH7WGrsY",
    "outputId": "2b6e43ef-bbd5-4325-f166-3d991084de3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4130, 2)\n",
      "Encoder initialized with:\n",
      "- Vocab size: 131\n",
      "- Embedding dim: 128\n",
      "- Latent dim: 128\n",
      "- 6 layers with 8 attention heads each\n",
      "\n",
      "Decoder initialized with:\n",
      "- Same vocab size: 131\n",
      "- Label embedding dim: 55\n",
      "- Hidden dim: 256\n",
      "- Using dropout: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/4474225.1.ece/ipykernel_3482703/2034855417.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier = torch.load(\"midi_gru_classifier_v2.pth\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Shared Parameters\n",
    "vocab_size = 131        # Number of unique MIDI tokens\n",
    "embedding_dim = 128     # Size of token embeddings\n",
    "max_len = 502           # Max sequence length (500 tokens + EOS + BOS)\n",
    "latent_dim = 128        # Latent space dimension\n",
    "num_heads = 8           # Number of attention heads\n",
    "num_layers = 6          # Number of transformer layers\n",
    "ff_dim = 512            # Feed-forward layer dimension\n",
    "dropout = 0.1           # Dropout rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Additional Decoder-specific Parameters\n",
    "label_dim = 55          # Dimension for label embeddings (# of composers)\n",
    "hidden_dim = 256        # Hidden dimension in decoder (d_hid in your code)\n",
    "\n",
    "print(data.shape)\n",
    "# Initialize Encoder\n",
    "encoder = Variational_Encoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    max_len=max_len,\n",
    "    latent_dim=latent_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    ff_dim=ff_dim\n",
    ").to(device)\n",
    "\n",
    "# Initialize Decoder\n",
    "decoder = Transformer_Decoder(\n",
    "    src_vocab_size=vocab_size,    # Same as encoder vocab size\n",
    "    tgt_vocab_size=vocab_size,    # Same unless you have different input/output vocabs\n",
    "    d_model=embedding_dim,        # Should match encoder's embedding_dim\n",
    "    nhead=num_heads,              # Same as encoder\n",
    "    d_hid=hidden_dim,             # Decoder-specific hidden dim\n",
    "    nlayers=num_layers,           # Same as encoder\n",
    "    latent_dim=latent_dim,        # Same as encoder\n",
    "    label_dim=label_dim,          # For composer/style conditioning\n",
    "    device=device,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "#load classifier\n",
    "#vocab_size=131, embed_dim=128, hidden_size=128, num_layers=2, num_classes=len(label_map)\n",
    "classifier = torch.load(\"midi_gru_classifier_v2.pth\", map_location=device)\n",
    "\n",
    "classifier.eval()\n",
    "classifier.to(device)\n",
    "\n",
    "# workout since classifier was trained with \n",
    "optimizer = optim.Adam([{'params': encoder.parameters(), 'lr': 1e-4},{'params': decoder.parameters(), 'lr': 3e-4}],weight_decay=1e-5) \n",
    "                       \n",
    "model = MIDIVAE(encoder,decoder,classifier,max_len)\n",
    "\n",
    "print(\"Encoder initialized with:\")\n",
    "print(f\"- Vocab size: {vocab_size}\")\n",
    "print(f\"- Embedding dim: {embedding_dim}\")\n",
    "print(f\"- Latent dim: {latent_dim}\")\n",
    "print(f\"- {num_layers} layers with {num_heads} attention heads each\")\n",
    "\n",
    "print(\"\\nDecoder initialized with:\")\n",
    "print(f\"- Same vocab size: {vocab_size}\")\n",
    "print(f\"- Label embedding dim: {label_dim}\")\n",
    "print(f\"- Hidden dim: {hidden_dim}\")\n",
    "print(f\"- Using dropout: {dropout}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  96%|█████████▌| 248/259 [14:44<00:39,  3.56s/it]"
     ]
    }
   ],
   "source": [
    "#def train_model(self, dataloader, optimizer, epochs=10):\n",
    "model.train_model(train_data,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
