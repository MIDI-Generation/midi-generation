{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mdZlKX0m42z3"
   },
   "outputs": [],
   "source": [
    "! CUDA_LAUNCH_BLOCKING=1\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "REST_ID = 128\n",
    "BOS_ID = 129\n",
    "EOS_ID = 130\n",
    "\n",
    "class MIDIVAE(nn.Module):\n",
    "  def __init__(self, encoder, decoder,classifier,max_length=502,device = 'cuda',kld_weight = 1.0, classifier_weight = 1.0):\n",
    "      super(MIDIVAE,self).__init__()\n",
    "\n",
    "      self.device = device\n",
    "      # untrained components\n",
    "      self.encoder = encoder.to(self.device)\n",
    "      self.decoder = decoder.to(self.device)  # Cross-Attention transformer to generate MIDI vectors\n",
    "      self.max_length = max_length\n",
    "      #workaround for cross-attention\n",
    "      self.num_memory_tokens = (encoder.latent_dim // 16)\n",
    "      self.memory_proj = nn.Sequential(\n",
    "            nn.Linear(encoder.latent_dim, decoder.d_model * self.num_memory_tokens),\n",
    "            nn.Unflatten(1, (self.num_memory_tokens, decoder.d_model))  # [B, num_tokens, d_model]\n",
    "        ).to(self.device)\n",
    "      # pre-trained classifier\n",
    "      self.classifier = classifier\n",
    "\n",
    "      #constants\n",
    "      self.kld_weight = kld_weight\n",
    "      self.classifier_weight = classifier_weight\n",
    "      self.BOS_ID = BOS_ID\n",
    "      self.EOS_ID = EOS_ID\n",
    "      self.REST_ID = REST_ID\n",
    "\n",
    "  def forward(self, x, label, teacher_forcing_ratio=0.5):\n",
    "        z, mean, logvar = self.encoder(x, label)\n",
    "\n",
    "        memory = self._prepare_memory(z.to(self.device)).to(self.device)\n",
    "\n",
    "        tgt = torch.full((1, x.shape[0]), self.BOS_ID, device=self.device)  # [1, batch_size]\n",
    "        generated_tokens = tgt\n",
    "        #print(f'generated tok shape { generated_tokens.shape}')\n",
    "        # logits_list = []\n",
    "      \n",
    "        for i in range(501): \n",
    "            if self.training and random.random() < teacher_forcing_ratio:\n",
    "                # Pass the ground truth (x) as the target for the next token\n",
    "                #print('------TEACHER FORCING--------')\n",
    "                #print(x.shape)\n",
    "                #print(x[:,:i+1].shape)\n",
    "                next_token_logits = self.decoder(\n",
    "                    tgt = x[:,:i+1].transpose(0,1),  # Use ground truth target token sequence\n",
    "                    memory=memory,  # Latent memory\n",
    "                    z=z,  # Latent representation from encoder\n",
    "                    label=label,  # Conditioning label\n",
    "                    teacher_forcing=True  # Enable teacher forcing\n",
    "                ).transpose(0, 1)\n",
    "            else:\n",
    "                #print('------ FREESTYLE--------')\n",
    "                #print(f'generated tok shape { generated_tokens.shape}')\n",
    "                next_token_logits = self.decoder(\n",
    "                    tgt=generated_tokens,  # Use generated tokens\n",
    "                    memory=memory,  # Latent memory\n",
    "                    z=z,  # Latent representation from encoder\n",
    "                    label=label,  # Conditioning label\n",
    "                    teacher_forcing=False  # Disable teacher forcing\n",
    "                ).transpose(0, 1)  # Adjust shape for further processing\n",
    "            #logits_list.append(next_token_logits)\n",
    "            next_token_logits = next_token_logits[-1, :, :]  # [1, batch_size, vocab_size]\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)  # [1, batch_size]\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=0)\n",
    "            # Force EOS token at max length (502 tokens)\n",
    "            if i == 501:\n",
    "                next_token.fill_(self.EOS_ID)  # Force EOS token at max_len\n",
    "                print(\"Forcing <EOS> token at 502nd position.\")\n",
    "                generated_tokens = torch.cat([generated_tokens, next_token], dim=0)\n",
    "                break\n",
    "            if (next_token == self.EOS_ID).all():\n",
    "                print(\"Forcing <EOS> token at max length.\")\n",
    "                break\n",
    "            #print(f'generate seq shape {generated_tokens.shape},next_token shape{next_token.shape}')\n",
    "        recon_midi = generated_tokens  # Final generated sequence of tokens\n",
    "        composer_pred = self.classifier(recon_midi.transpose(0,1))\n",
    "\n",
    "        return recon_midi,mean, logvar, composer_pred\n",
    "\n",
    "  def train_model(self, dataloader, optimizer, epochs=10):\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x, label in tqdm(dataloader,desc = f\"Training Epoch {epoch + 1}\"):\n",
    "                x, label = x.to(self.device), label.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                recon_x, mu, logvar, pred = self(x, label)\n",
    "                #recon_x, token_logits,mu, logvar = self(x, label)\n",
    "                \n",
    "                # Loss calculations\n",
    "                kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                #print(f'x data type {x.dtype} recon data type {recon_x.dtype}')\n",
    "                #print(f'recon shape{recon_x.transpose(0,1).shape},x shape {x.shape}')\n",
    "                recon_loss = F.mse_loss(\n",
    "                    recon_x.transpose(0,1).to(torch.float32),  # [batch,seq_len]\n",
    "                    x.to(torch.float32)  # [batch, seq_len]\n",
    "                )\n",
    "                \n",
    "                cls_loss = F.cross_entropy(pred, label)\n",
    "\n",
    "                loss = recon_loss + self.kld_weight*kld + self.classifier_weight*cls_loss\n",
    "                #loss = recon_loss + self.kld_weight*kld\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "  def _prepare_memory(self, z):\n",
    "        memory = self.memory_proj(z).to(self.device)  # [B, num_tokens, d_model]\n",
    "        return memory.transpose(0, 1).to(self.device)\n",
    "\n",
    "  def generate(self, z, label, temperature=1.0):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            memory = self._prepare_memory(z)\n",
    "            return self.decoder.generate(\n",
    "                memory=memory,\n",
    "                z=z,\n",
    "                label=label,\n",
    "                max_len=self.max_length,\n",
    "                temperature=temperature,\n",
    "                bos_id=self.bos_id,\n",
    "                eos_id=self.eos_id\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rSBeiQ44HXNq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Token_Embedding(nn.Module):\n",
    "  def __init__(self,vocab_size,embedding_dim):\n",
    "    super(Token_Embedding,self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.embedding(x)\n",
    "\n",
    "class Pos_Embedding(nn.Module):\n",
    "  def __init__(self,max_len,embedding_dim):\n",
    "    super(Pos_Embedding,self).__init__()\n",
    "    self.pos_embedding = nn.Embedding(max_len,embedding_dim)\n",
    "\n",
    "  def forward(self,x):\n",
    "    seq_len = x.size(1)\n",
    "    pos_ids = torch.arange(seq_len,device=x.device).unsqueeze(0)\n",
    "    return self.pos_embedding(pos_ids)\n",
    "\n",
    "class Transformer_Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, num_layers, ff_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ff_dim,\n",
    "                batch_first=True  # ← Critical for your input shape\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class LatentSpace_Mean_Log(nn.Module):\n",
    "  def __init__(self,embedding_dim,latent_dim):\n",
    "    super(LatentSpace_Mean_Log,self).__init__()\n",
    "    self.fc_mu = nn.Linear(embedding_dim,latent_dim)\n",
    "    self.fc_logvar = nn.Linear(embedding_dim,latent_dim)\n",
    "\n",
    "  def forward(self,x):\n",
    "    mu = self.fc_mu(x)\n",
    "    logvar = self.fc_logvar(x)\n",
    "\n",
    "    return mu,logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Wn_47aTe66-3"
   },
   "outputs": [],
   "source": [
    "class Variational_Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_len=502, latent_dim=64,\n",
    "                 num_heads=8, num_layers=6, ff_dim=512, label_dim=1):\n",
    "        super().__init__()\n",
    "        assert max_len >= 502, \"max_len must cover BOS+EOS+500 tokens\"\n",
    "        self.token_embedding = Token_Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_embedding = Pos_Embedding(max_len, embedding_dim)\n",
    "        self.encoder = Transformer_Encoder(embedding_dim, num_heads, num_layers, ff_dim)\n",
    "        self.latent_proj = LatentSpace_Mean_Log(embedding_dim, latent_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        # Optional label conditioning\n",
    "        self.label_projection = nn.Embedding(label_dim, embedding_dim) if label_dim > 0 else None\n",
    "        self.device = 'cuda'\n",
    "        # initialization\n",
    "        #token embedding\n",
    "        torch.nn.init.xavier_uniform_(self.token_embedding.embedding.weight)\n",
    "        #posembedding\n",
    "        torch.nn.init.xavier_uniform_(self.pos_embedding.pos_embedding.weight)\n",
    "        #latentspacemeanlog\n",
    "        torch.nn.init.xavier_uniform_(self.latent_proj.fc_mu.weight)\n",
    "        torch.nn.init.zeros_(self.latent_proj.fc_mu.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.latent_proj.fc_logvar.weight)\n",
    "        torch.nn.init.zeros_(self.latent_proj.fc_logvar.bias)\n",
    "        #labelprojection\n",
    "        torch.nn.init.xavier_uniform_(self.label_projection.weight)\n",
    "\n",
    "        for layer in self.encoder.layers:\n",
    "            # Initialize the multi-head attention layers\n",
    "            torch.nn.init.xavier_uniform_(layer.self_attn.in_proj_weight)  # Query, Key, Value weights\n",
    "            torch.nn.init.xavier_uniform_(layer.self_attn.out_proj.weight)  # Output projection\n",
    "    \n",
    "            \n",
    "            # Initialize feed-forward network weights\n",
    "            torch.nn.init.xavier_uniform_(layer.linear1.weight)  # First linear layer of FFN\n",
    "            torch.nn.init.xavier_uniform_(layer.linear2.weight)  # Second linear layer of FFN\n",
    "            \n",
    "            # Initialize layer normalization weights\n",
    "            torch.nn.init.constant_(layer.norm1.weight, 1)  # Norm1 (self-attention layer)\n",
    "            torch.nn.init.constant_(layer.norm2.weight, 1)  # Norm2 (feed-forward layer)\n",
    "            \n",
    "            # Initialize biases\n",
    "            torch.nn.init.zeros_(layer.self_attn.in_proj_bias)  # Bias for attention weights\n",
    "            torch.nn.init.zeros_(layer.self_attn.out_proj.bias)  # Bias for attention output\n",
    "            torch.nn.init.zeros_(layer.linear1.bias)  # Bias for first linear layer\n",
    "            torch.nn.init.zeros_(layer.linear2.bias)  # Bias for second linear layer\n",
    "            torch.nn.init.zeros_(layer.norm1.bias)  # Bias for Norm1\n",
    "            torch.nn.init.zeros_(layer.norm2.bias)  # Bias for Norm2\n",
    "            \n",
    "    def forward(self, x, label=None):\n",
    "        # Input x: [batch_size, 502]\n",
    "        #print(f\"Input shape: {x.shape}\")  # Should be [batch, 502]\n",
    "        #print(f\"Input device: {x.device}\")\n",
    "        tok_emb = self.token_embedding(x)  # [B, 502, D]\n",
    "        pos_emb = self.pos_embedding(x)    # [B, 502, D]\n",
    "        embeddings = tok_emb + pos_emb\n",
    "\n",
    "        # Inject label info (if provided)\n",
    "        if self.label_projection and label is not None:\n",
    "            label_emb = self.label_projection(label.to(self.device)).unsqueeze(1)  # [B, 1, D]\n",
    "            #shape of label_emb = [16,1,128]\n",
    "            #shape of embedding = [16,502,128]\n",
    "            #x.size(1) is 502\n",
    "            temp = label_emb.expand(-1, x.size(1), -1)  # [B, 502, D]\n",
    "            embeddings = embeddings + temp\n",
    "        \n",
    "        # Transformer process\n",
    "        output = self.encoder(embeddings)  # [B, 502, D]\n",
    "\n",
    "        # Pool and project to latent space\n",
    "        pooled = output.mean(dim=1)  # [B, D]\n",
    "        mu, logvar = self.latent_proj(pooled)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "\n",
    "        return z, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MTfiJo80fvcn"
   },
   "outputs": [],
   "source": [
    "#!!! Adapted from HW5!!!\n",
    "\n",
    "from torch import nn, Tensor\n",
    "import torch\n",
    "import math\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Cross-Attention Transformer Decoder\n",
    "class Transformer_Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 d_hid: int,\n",
    "                 nlayers: int,\n",
    "                 latent_dim: int,\n",
    "                 label_dim: int,\n",
    "                 dropout: float = 0.5,\n",
    "                 device = 'cuda'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.device = device\n",
    "        # discrete latent tokens\n",
    "        # self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        # predicted MIDI tokens\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        self.memory_proj = nn.Linear(latent_dim, d_model)\n",
    "\n",
    "        \n",
    "        # Decoder (self-attention and cross-attention)\n",
    "        dec_layer = TransformerDecoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(dec_layer, nlayers)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        #weight modulation layers\n",
    "        self.z_scale = nn.Linear(latent_dim, d_model)\n",
    "        self.z_shift = nn.Linear(latent_dim, d_model)\n",
    "        # label dim should be B x\n",
    "        #label conditioning layers for cross attention\n",
    "        self.label_projection = nn.Embedding(label_dim,d_model)\n",
    "        self.label_attn = nn.MultiheadAttention(d_model,nhead,dropout=dropout)\n",
    "        # Project z to scaling factors\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.tgt_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        torch.nn.init.xavier_uniform_(self.tgt_embedding.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.label_projection.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.memory_proj.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.z_scale.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.z_shift.weight)\n",
    "        self.memory_proj.bias.data.zero_()\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.z_scale.bias.data.zero_()\n",
    "        self.z_shift.bias.data.zero_()\n",
    "        for layer in self.transformer_decoder.layers:\n",
    "            # Initialize multi-head attention weights\n",
    "            torch.nn.init.xavier_uniform_(layer.self_attn.in_proj_weight)\n",
    "            torch.nn.init.xavier_uniform_(layer.self_attn.out_proj.weight)\n",
    "            torch.nn.init.xavier_uniform_(layer.multihead_attn.in_proj_weight)\n",
    "            torch.nn.init.xavier_uniform_(layer.multihead_attn.out_proj.weight)\n",
    "        \n",
    "        # Initialize feed-forward network weights\n",
    "        torch.nn.init.xavier_uniform_(layer.linear1.weight)\n",
    "        torch.nn.init.xavier_uniform_(layer.linear2.weight)\n",
    "        \n",
    "        # Initialize layer norms\n",
    "        torch.nn.init.constant_(layer.norm1.weight, 1)\n",
    "        torch.nn.init.constant_(layer.norm2.weight, 1)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.label_attn.in_proj_weight)\n",
    "        torch.nn.init.xavier_uniform_(self.label_attn.out_proj.weight)\n",
    "\n",
    "    def forward(self, tgt: Tensor,memory: Tensor,z : Tensor, label:Tensor,teacher_forcing=True):\n",
    "        # Scale and Embed the memory sequence\n",
    "        #print(f'Memory Shape{memory.shape}')\n",
    "        memory = self.memory_proj(memory) * math.sqrt(self.d_model)\n",
    "        # Add positional encoding\n",
    "        memory = self.pos_encoder(memory)\n",
    "        \n",
    "        # teacher forcing so that during training we can train well but also\n",
    "        # autoregressively predict during evaluation\n",
    "        #print(f'tgt shape {tgt.shape}')\n",
    "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        #print(f'tgt emb{tgt_emb.shape}')\n",
    "        shift = self.z_shift(z)\n",
    "        scale = self.z_scale(z)\n",
    "        #print(f'shift{shift.shape}')\n",
    "        #print(f'scale{scale.shape}')\n",
    "        #print(f'tgt shape{tgt_emb.shape} shift shape {shift.shape} scale shape {scale.shape}')\n",
    "        tgt_emb = tgt_emb * scale.unsqueeze(0) + shift.unsqueeze(0)\n",
    "    \n",
    "        label_emb = self.label_projection(label).unsqueeze(0)\n",
    "        label_emb = label_emb.expand(tgt_emb.shape[0], -1, -1)  # (batch_size,seq_len, d_model)\n",
    "        tgt_emb = self.label_attn(query=tgt_emb, key=label_emb, value=label_emb)[0]\n",
    "            \n",
    "        # Create the mask\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(len(tgt_emb)).to(self.device)\n",
    "        # Pass them through the transformer\n",
    "        output = self.transformer_decoder(tgt=tgt_emb, memory=memory, tgt_mask=tgt_mask)\n",
    "        # Apply the linear layer\n",
    "        output = self.linear(output)\n",
    "        output = output.transpose(0, 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def generate(self, memory, z, label, max_len=502, temperature=1.0, bos_id=129, eos_id=130):\n",
    "        batch_size = z.size(0)\n",
    "        device = self.device\n",
    "    \n",
    "        # 1. Start with [BOS] token for all sequences in batch\n",
    "        tgt = torch.full((1, batch_size), bos_id, device=device)  # [1, batch_size]\n",
    "        # 2. Project and encode memory\n",
    "        memory = self.memory_proj(memory) * math.sqrt(self.d_model)\n",
    "        memory = self.pos_encoder(memory)\n",
    "\n",
    "        logits_list = []\n",
    "        for i in range(max_len-1):\n",
    "            # 3. Forward pass\n",
    "            logits = self(\n",
    "                tgt=tgt.transpose(0, 1),  # Transformer expects [seq_len, batch_size]\n",
    "                memory=memory,\n",
    "                z=z,\n",
    "                label=label,\n",
    "                teacher_forcing=False\n",
    "            ) # Get logits for last token only [batch_size, vocab_size]\n",
    "            logits_list.append(logits)\n",
    "            logits = logits.transpose(0, 1)[-1,:,:]\n",
    "            # 4. Sample next token\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples= 1).transpose(0, 1)  # [1, batch_size]\n",
    "            # 5. Append next token\n",
    "            tgt = torch.cat([tgt, next_token], dim=0)  # along seq_len (dim=0)\n",
    "            if i == max_len - 1:\n",
    "                next_token.fill_(eos_id)  # Force EOS token at max_len\n",
    "                print(\"Forcing <EOS> token at 502nd position.\")\n",
    "                tgt = torch.cat([tgt, next_token], dim=0)\n",
    "                break\n",
    "            # 6. Early stopping if all samples predict EOS\n",
    "            if (next_token == eos_id).all():\n",
    "                break\n",
    "        #print(f'tgt shape{tgt.shape}')\n",
    "        return tgt,torch.cat(logits_list, dim=0)  # [seq_len-1, batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vqa0QC59r17",
    "outputId": "605f2716-c6d8-4cf5-c748-f4897f878977"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6wuW-wFzEBts"
   },
   "outputs": [],
   "source": [
    "file_directory = '/content/drive/MyDrive/maestro_token_sequences.csv' # colab\n",
    "file_directory = '/projectnb/ec523/projects/proj_MIDIgen/maestro_token_sequences.csv' #scc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9EDS3PnCFnQg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "def string_to_vector(seq_str):\n",
    "    return [int(x) for x in seq_str.split()]\n",
    "\n",
    "def prepare_data(file, augmentations=False, collaborations=False,split='train'):\n",
    "    # Read the data\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # only read train data\n",
    "    if(split != 'all'):\n",
    "      df = df[df['split'] == split].copy()\n",
    "\n",
    "    # Convert sequences to vectors\n",
    "    df['sequence_vector'] = df['sequence'].apply(string_to_vector)\n",
    "\n",
    "    # Apply filters\n",
    "    if (not collaborations):\n",
    "        df = df[~df['composer'].str.contains('/', na=False)].copy()\n",
    "\n",
    "    if (not augmentations):\n",
    "        df = df[df['transposition amount'] == 0].copy()\n",
    "\n",
    "    # Create labels (assuming composer_label_dict exists)\n",
    "    clean_df = df.copy()  # Final cleaned version\n",
    "    composer_list = sorted(list(set(clean_df['composer'])))  # Convert to sorted list for consistent ordering\n",
    "    num_composers = len(composer_list)\n",
    "    print(f\"Unique composers: {composer_list}\")\n",
    "    print(f\"Total composers: {num_composers}\")\n",
    "\n",
    "    # Create proper label dictionary\n",
    "    composer_label_dict = {composer: idx for idx, composer in enumerate(composer_list)}\n",
    "    index_composer_dict = {idx: composer for composer, idx in composer_label_dict.items()}\n",
    "    clean_df['label'] = clean_df['composer'].map(composer_label_dict)\n",
    "\n",
    "    # Select only the two columns we want\n",
    "    data = clean_df[['sequence_vector', 'label']].copy()\n",
    "\n",
    "    return data,composer_label_dict,index_composer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FMCJfN0PiOuD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "class MIDI_Dataset(Dataset):\n",
    "  def __init__(self,sequences,labels):\n",
    "    self.sequences = sequences\n",
    "    self.labels = labels\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.sequences)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "      # Convert to tensor directly (no padding needed)\n",
    "      seq = torch.tensor(self.sequences[idx], dtype=torch.long)\n",
    "\n",
    "      if self.labels is not None:\n",
    "          label = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "          return seq, label\n",
    "      return seq\n",
    "def create_MIDI_Dataloaders(train_data, batch_size=16):\n",
    "\n",
    "  # Create datasets\n",
    "  train_dataset = MIDI_Dataset(\n",
    "      sequences=train_data['sequence_vector'].tolist(),\n",
    "      labels=train_data['label']\n",
    "  )\n",
    "\n",
    "  # Create dataloaders\n",
    "  train_loader = DataLoader(\n",
    "      train_dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      pin_memory=True,\n",
    "      num_workers=4  # Parallel loading\n",
    "  )\n",
    "\n",
    "  return train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique composers: ['Alexander Scriabin', 'César Franck', 'Felix Mendelssohn', 'Franz Liszt', 'Franz Schubert', 'Frédéric Chopin', 'Johann Sebastian Bach', 'Johannes Brahms', 'Joseph Haydn', 'Ludwig van Beethoven', 'Mily Balakirev', 'Robert Schumann', 'Sergei Rachmaninoff', 'Wolfgang Amadeus Mozart']\n",
      "Total composers: 14\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "data,composer_to_label_map,inv_map = prepare_data(file_directory,augmentations=False,split='validation')\n",
    "train_data = create_MIDI_Dataloaders(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Tried 256 hidden layers initiallty, but overfitted... :( (Also increased dropout from 0.3 to 0.5)\n",
    "class MidiGRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=131, embed_dim=128, hidden_size=128, num_layers=2, num_classes=34, bidirectional=True, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1  \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, num_layers,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout if num_layers > 1 else 0,\n",
    "                          bidirectional=bidirectional)\n",
    "        self.norm = nn.LayerNorm(hidden_size * self.num_directions)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.num_directions, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.norm(out[:, -1, :])\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "sdooHH7WGrsY",
    "outputId": "2b6e43ef-bbd5-4325-f166-3d991084de3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4130, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/4474225.1.ece/ipykernel_3501336/2034855417.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier = torch.load(\"midi_gru_classifier_v2.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder initialized with:\n",
      "- Vocab size: 131\n",
      "- Embedding dim: 128\n",
      "- Latent dim: 128\n",
      "- 6 layers with 8 attention heads each\n",
      "\n",
      "Decoder initialized with:\n",
      "- Same vocab size: 131\n",
      "- Label embedding dim: 55\n",
      "- Hidden dim: 256\n",
      "- Using dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Shared Parameters\n",
    "vocab_size = 131        # Number of unique MIDI tokens\n",
    "embedding_dim = 128     # Size of token embeddings\n",
    "max_len = 502           # Max sequence length (500 tokens + EOS + BOS)\n",
    "latent_dim = 128        # Latent space dimension\n",
    "num_heads = 8           # Number of attention heads\n",
    "num_layers = 6          # Number of transformer layers\n",
    "ff_dim = 512            # Feed-forward layer dimension\n",
    "dropout = 0.1           # Dropout rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Additional Decoder-specific Parameters\n",
    "label_dim = 55          # Dimension for label embeddings (# of composers)\n",
    "hidden_dim = 256        # Hidden dimension in decoder (d_hid in your code)\n",
    "\n",
    "print(data.shape)\n",
    "# Initialize Encoder\n",
    "encoder = Variational_Encoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    max_len=max_len,\n",
    "    latent_dim=latent_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    ff_dim=ff_dim\n",
    ").to(device)\n",
    "\n",
    "# Initialize Decoder\n",
    "decoder = Transformer_Decoder(\n",
    "    src_vocab_size=vocab_size,    # Same as encoder vocab size\n",
    "    tgt_vocab_size=vocab_size,    # Same unless you have different input/output vocabs\n",
    "    d_model=embedding_dim,        # Should match encoder's embedding_dim\n",
    "    nhead=num_heads,              # Same as encoder\n",
    "    d_hid=hidden_dim,             # Decoder-specific hidden dim\n",
    "    nlayers=num_layers,           # Same as encoder\n",
    "    latent_dim=latent_dim,        # Same as encoder\n",
    "    label_dim=label_dim,          # For composer/style conditioning\n",
    "    device=device,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "\n",
    "#load classifier\n",
    "#vocab_size=131, embed_dim=128, hidden_size=128, num_layers=2, num_classes=len(label_map)\n",
    "classifier = torch.load(\"midi_gru_classifier_v2.pth\", map_location=device)\n",
    "\n",
    "classifier.eval()\n",
    "classifier.to(device)\n",
    "\n",
    "# workout since classifier was trained with \n",
    "optimizer = optim.Adam([{'params': encoder.parameters(), 'lr': 1e-4},{'params': decoder.parameters(), 'lr': 3e-4}],weight_decay=1e-5) \n",
    "                       \n",
    "model = MIDIVAE(encoder,decoder,classifier,max_len)\n",
    "\n",
    "print(\"Encoder initialized with:\")\n",
    "print(f\"- Vocab size: {vocab_size}\")\n",
    "print(f\"- Embedding dim: {embedding_dim}\")\n",
    "print(f\"- Latent dim: {latent_dim}\")\n",
    "print(f\"- {num_layers} layers with {num_heads} attention heads each\")\n",
    "\n",
    "print(\"\\nDecoder initialized with:\")\n",
    "print(f\"- Same vocab size: {vocab_size}\")\n",
    "print(f\"- Label embedding dim: {label_dim}\")\n",
    "print(f\"- Hidden dim: {hidden_dim}\")\n",
    "print(f\"- Using dropout: {dropout}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/259 [00:00<?, ?it/s]\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1729647429097/work/aten/src/ATen/native/cuda/Indexing.cu:1255: indexSelectSmallIndex: block: [0,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#def train_model(self, dataloader, optimizer, epochs=10):\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 99\u001b[0m, in \u001b[0;36mMIDIVAE.train_model\u001b[0;34m(self, dataloader, optimizer, epochs)\u001b[0m\n\u001b[1;32m     96\u001b[0m x, label \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), label\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 99\u001b[0m recon_x, mu, logvar, pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m#recon_x, token_logits,mu, logvar = self(x, label)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Loss calculations\u001b[39;00m\n\u001b[1;32m    103\u001b[0m kld \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m logvar \u001b[38;5;241m-\u001b[39m mu\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m logvar\u001b[38;5;241m.\u001b[39mexp())\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m, in \u001b[0;36mMIDIVAE.forward\u001b[0;34m(self, x, label, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, label, teacher_forcing_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m---> 39\u001b[0m       z, mean, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m       memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_memory(z\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     43\u001b[0m       tgt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBOS_ID, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# [1, batch_size]\u001b[39;00m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 67\u001b[0m, in \u001b[0;36mVariational_Encoder.forward\u001b[0;34m(self, x, label)\u001b[0m\n\u001b[1;32m     64\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings \u001b[38;5;241m+\u001b[39m temp\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Transformer process\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, 502, D]\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Pool and project to latent space\u001b[39;00m\n\u001b[1;32m     70\u001b[0m pooled \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B, D]\u001b[39;00m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m, in \u001b[0;36mTransformer_Encoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 33\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/transformer.py:904\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    900\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m    903\u001b[0m         x\n\u001b[0;32m--> 904\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m     )\n\u001b[1;32m    906\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/transformer.py:918\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    913\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    917\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 918\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/modules/activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1343\u001b[0m         query,\n\u001b[1;32m   1344\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/functional.py:6097\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   6094\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   6095\u001b[0m         in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6096\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 6097\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6098\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6099\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   6100\u001b[0m         q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6101\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/share/pkg.8/academic-ml/spring-2025/install/spring-2025-pyt/lib/python3.11/site-packages/torch/nn/functional.py:5501\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   5498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m v:\n\u001b[1;32m   5499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m q \u001b[38;5;129;01mis\u001b[39;00m k:\n\u001b[1;32m   5500\u001b[0m         \u001b[38;5;66;03m# self-attention\u001b[39;00m\n\u001b[0;32m-> 5501\u001b[0m         proj \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5502\u001b[0m         \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[1;32m   5503\u001b[0m         proj \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   5504\u001b[0m             proj\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m3\u001b[39m, E))\n\u001b[1;32m   5505\u001b[0m             \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5508\u001b[0m             \u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m   5509\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "#def train_model(self, dataloader, optimizer, epochs=10):\n",
    "model.train_model(train_data,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
