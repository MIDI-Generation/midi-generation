{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIOlhGi5PXWu"
      },
      "outputs": [],
      "source": [
        "#installations\n",
        "!pip install torchaudio\n",
        "!sudo apt install -y fluidsynth\n",
        "!pip install --upgrade pyfluidsynth\n",
        "!pip install pretty_midi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUm4c7k-4HTJ"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "from itertools import product\n",
        "import os\n",
        "import music21\n",
        "import pretty_midi\n",
        "from music21 import midi\n",
        "import pandas as pd\n",
        "from IPython.display import Audio\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import statistics\n",
        "import math, random, time\n",
        "from IPython.display import Audio\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import torchaudio\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import scipy\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4yTK8yu6ZAK"
      },
      "outputs": [],
      "source": [
        "#mount drive, establish data directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_directory = '/content/drive/MyDrive/maestro-v2.0.0-csv/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7eboztT5iGE"
      },
      "outputs": [],
      "source": [
        "# generate token dict\n",
        "def generate_token_dict(time_step=0.01, max_shift_secs=1.0, num_velocity_bins=32):\n",
        "\n",
        "    token_dict = {}\n",
        "    num_time_bins = int(max_shift_secs / time_step)\n",
        "    idx = 0\n",
        "\n",
        "    #time-shift tokens\n",
        "    for t in range(1, num_time_bins + 1):\n",
        "        token_dict[f\"time_shift_{t}\"] = idx\n",
        "        idx += 1\n",
        "\n",
        "    #velocity tokens, by bin index 0..31\n",
        "    for b in range(num_velocity_bins):\n",
        "        token_dict[f\"velocity_{b}\"] = idx\n",
        "        idx += 1\n",
        "\n",
        "    # note-on & note-off tokens\n",
        "    for p in range(128):\n",
        "        token_dict[f\"note_on_pitch_{p}\"]  = idx; idx += 1\n",
        "    for p in range(128):\n",
        "        token_dict[f\"note_off_pitch_{p}\"] = idx; idx += 1\n",
        "\n",
        "    #special tokens\n",
        "    token_dict[\"<bos>\"]  = idx; idx += 1\n",
        "    token_dict[\"<eos>\"]  = idx\n",
        "\n",
        "    inverse_token_dict = {v:k for k,v in token_dict.items()}\n",
        "    return token_dict, inverse_token_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI-BlGUpuFWt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import pretty_midi\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MaestroDataset(Dataset):\n",
        "    def __init__(self, split, token_dict=None, directory='/content/drive/MyDrive/maestro-v2.0.0-csv/'):\n",
        "        import pandas as pd\n",
        "\n",
        "  #define attributes: train/test/validation split, token dictionary, target directory\n",
        "        self.split = split\n",
        "        self.token_dict = token_dict\n",
        "        self.directory = directory.rstrip('/') + '/'\n",
        "        # read the CSV and filter  split\n",
        "        df = pd.read_csv(os.path.join(self.directory, 'maestro-v2.0.0.csv'))\n",
        "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "  #create token sequences for a single work\n",
        "    def create_token_sequences(self, pm: pretty_midi.PrettyMIDI, time_step:float = 0.01, token_dict:dict = None, velocity_bins:np.ndarray = None, fs: int = 16, seq_len: int = 500, add_bos_eos: bool = True):\n",
        "        events = []\n",
        "\n",
        "        for inst in pm.instruments:\n",
        "          for note in inst.notes:\n",
        "            events.append(('on', note.start, note.pitch, note.velocity))\n",
        "            events.append(('off', note.end, note.pitch, note.velocity))\n",
        "        events.sort(key=lambda s: s[1])\n",
        "\n",
        "        seq = [token_dict['<bos>']]\n",
        "        prev_time = 0.0\n",
        "\n",
        "        for on_off, time, pitch, velocity in events:\n",
        "            delta = time - prev_time\n",
        "            num_steps = int(round(delta/time_step))\n",
        "            for i in range(num_steps):\n",
        "              max_bin = int(1.0/ time_step)\n",
        "              seq.append(token_dict[f\"time_shift_{min(i+1, max_bin)}\"])\n",
        "            prev_time += num_steps * time_step\n",
        "            if on_off == 'on':\n",
        "              quant_velocity = int(np.argmin(np.abs(velocity_bins - velocity)))\n",
        "              seq.append(token_dict[f'velocity_{quant_velocity}'])\n",
        "              seq.append(token_dict[f'note_on_pitch_{pitch}'])\n",
        "\n",
        "            else:\n",
        "              seq.append(token_dict[f'note_off_pitch_{pitch}'])\n",
        "        seq.append(token_dict['<eos>'])\n",
        "\n",
        "        blocks = []\n",
        "\n",
        "        for start in range(0, len(seq), seq_len):\n",
        "          end = start + seq_len\n",
        "          if end > len(seq):\n",
        "            break\n",
        "          block = seq[start:end]\n",
        "          blocks.append(block)\n",
        "\n",
        "        return blocks\n",
        "\n",
        "#create token sequences for entire dataset\n",
        "    def create_all_token_sequences(self, fs:int = 16, seq_len:int = 500):\n",
        "        split_dir = os.path.join(self.directory, self.split)\n",
        "        print(\"-> files in\", split_dir, \"=\", os.listdir(split_dir)[:10])\n",
        "        all_seqs = []\n",
        "\n",
        "        for file_name in tqdm(os.listdir(split_dir)):\n",
        "            if not file_name.lower().endswith(('.mid', '.midi')):\n",
        "                continue\n",
        "            path = os.path.join(split_dir, file_name)\n",
        "            try:\n",
        "                pm = pretty_midi.PrettyMIDI(path)\n",
        "                pm.remove_invalid_notes()\n",
        "                base_name = os.path.basename(file_name)\n",
        "                compare = self.df['midi_filename'].apply(os.path.basename) == base_name\n",
        "                print(\"    looking for\", file_name, \" -> matched rows:\", compare.sum())\n",
        "                if not compare.any():\n",
        "                    print(f\"Warning: MIDI file not found in CSV: {path}\")\n",
        "                    continue\n",
        "                composer_label = self.df.loc[compare, 'canonical_composer'].values[0]\n",
        "                title_label = self.df.loc[compare, 'canonical_title'].values[0]\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {file_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "            seqs = self.create_token_sequences(pm, fs=fs, seq_len=seq_len, token_dict=self.token_dict, velocity_bins=np.linspace(0,127,32))\n",
        "            for s in seqs:\n",
        "                all_seqs.append({'title': title_label, 'sequence': s, 'composer': composer_label, 'transposition amount': 0})\n",
        "                for pshift, aug in self.augment_idxs(s):\n",
        "                      all_seqs.append({'title': title_label, 'sequence': aug, 'composer': composer_label, 'transposition amount': pshift})\n",
        "        return all_seqs\n",
        "\n",
        "#augment data via transposition\n",
        "    def augment_idxs(self, seq):\n",
        "        augmented_shift = []\n",
        "        for shift in range(-12, +13):\n",
        "            out = []\n",
        "            for idx in seq:\n",
        "                # only shift pitches, ignore special tokens\n",
        "                if 0 <= idx < 128:\n",
        "                    n = idx + shift\n",
        "                    n = max(0, min(127, n))\n",
        "                    out.append(n)\n",
        "                else:\n",
        "                    out.append(idx)\n",
        "            augmented_shift.append((shift, out))\n",
        "        return augmented_shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYQq6yOlxD6I"
      },
      "outputs": [],
      "source": [
        "#generate csv with MIDI tokens based on data with initial suggested splits\n",
        "token_dict, inverse_token_dict = generate_token_dict()\n",
        "csv_rows = []\n",
        "\n",
        "for phase in ['train', 'test', 'validation']:\n",
        "    ds = MaestroDataset(phase, token_dict,\n",
        "                        directory='/content/drive/MyDrive/maestro-v2.0.0-csv/')\n",
        "    seqs = ds.create_all_token_sequences(fs=16, seq_len=500)\n",
        "    for seq in seqs:\n",
        "        seq['split'] = phase\n",
        "        seq['sequence'] = ' '.join(map(str, seq['sequence']))\n",
        "\n",
        "    csv_rows.extend(seqs)\n",
        "df = pd.DataFrame(csv_rows)\n",
        "dir_csv = '/content/drive/MyDrive/maestro-v2.0.0-csv/'\n",
        "df.to_csv('maestro_token_sequences_bigger_vocab.csv', index=False)\n",
        "print(f'Wrote {len(df)} rows to {dir_csv}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S33JCGIF_8Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "\n",
        "#resplit data in different ways indicated by functions below\n",
        "input_csv = 'maestro_token_sequences_bigger_vocab.csv'\n",
        "\n",
        "def prepare_data(file, augmentations=False, collaborations=False):\n",
        "    # Read the data\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    df['sequence_vector'] = df['sequence'].apply(string_to_vector)\n",
        "\n",
        "#Apply filters\n",
        "    if not collaborations:\n",
        "        df = df[~df['composer'].str.contains('/', na=False)].copy()\n",
        "    if not augmentations:\n",
        "        df = df[df['transposition amount'] == 0].copy()\n",
        "\n",
        "#Create labels (assuming composer_label_dict exists)\n",
        "    clean_df = df.copy()  #don't mutate the input\n",
        "    composer_list = sorted(list(set(clean_df['composer'])))  # convert to sorted list for consistent ordering\n",
        "    num_composers = len(composer_list)\n",
        "    print(f\"Unique composers: {composer_list}\")\n",
        "    print(f\"Total composers: {num_composers}\")\n",
        "\n",
        "    # Create proper label dictionary\n",
        "    composer_label_dict = {composer: idx for idx, composer in enumerate(composer_list)}\n",
        "    index_composer_dict = {idx: composer for composer, idx in composer_label_dict.items()}\n",
        "    clean_df['label'] = clean_df['composer'].map(composer_label_dict)\n",
        "\n",
        "#Select only the necessary columnns\n",
        "    data = clean_df[['sequence_vector', 'composer', 'label']].copy()\n",
        "\n",
        "    return data,composer_label_dict,index_composer_dict\n",
        "\n",
        "def string_to_vector(seq_str):\n",
        "    return [int(x) for x in seq_str.split()]\n",
        "\n",
        "def cross_validation_splits(data, labels, max_folds=5, random_state=None):\n",
        "    #place data into different split containers\n",
        "    splits = {}\n",
        "\n",
        "    for k in range(2, max_folds + 1):\n",
        "        for bool in (False, True):\n",
        "            kf = KFold(n_splits=k, shuffle=bool, random_state=random_state)\n",
        "            split_indices = kf.split(data, labels)\n",
        "            splits[f'{k}-fold' + ('-shuffled' if bool else '')] = list(split_indices)\n",
        "    return splits\n",
        "\n",
        "#60-30-10 split\n",
        "def split_data(features, labels):\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        features,\n",
        "        labels,\n",
        "        test_size=0.4,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    X_test, X_val, y_test, y_val = train_test_split(\n",
        "        X_temp,\n",
        "        y_temp,\n",
        "        test_size=0.25,\n",
        "        stratify=y_temp,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, X_val, y_val\n",
        "\n",
        "#k-fold cross validation\n",
        "def build_fold_data(data, labels, splits):\n",
        "    fold_data = {}\n",
        "    for key, idx_vals in splits.items():\n",
        "        fold_data[key] = []\n",
        "        for train_idx, test_idx in idx_vals:\n",
        "            data_train = data.iloc[train_idx].reset_index(drop=True)\n",
        "            data_test = data.iloc[test_idx].reset_index(drop=True)\n",
        "            label_train = labels.iloc[train_idx].reset_index(drop=True)\n",
        "            label_test = labels.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "            fold_data[key].append({\n",
        "                'training data': data_train,\n",
        "                'test data': data_test,\n",
        "                'training labels': label_train,\n",
        "                'test labels': label_test\n",
        "            })\n",
        "    return fold_data\n",
        "\n",
        "def export_all_folds(fold_data, output_path='all_folds.csv'):\n",
        "    all_rows = []\n",
        "    for split, folds in fold_data.items():\n",
        "        for i, fold in enumerate(folds, start=1):\n",
        "            df_train = fold['training data'].copy()\n",
        "            df_train['label'] = fold['training labels']\n",
        "            df_train['set'] = 'train'\n",
        "            df_train['split'] = split\n",
        "            all_rows.append(df_train)\n",
        "\n",
        "            df_test = fold['test data'].copy()\n",
        "            df_test['label'] = fold['test labels']\n",
        "            df_test['set'] = 'test'\n",
        "            df_test['split'] = split\n",
        "            all_rows.append(df_test)\n",
        "\n",
        "    whole_df = pd.concat(all_rows, ignore_index=True)\n",
        "    whole_df.to_csv(\"fold_train.csv.gz\", index=False, compression=\"gzip\")\n",
        "    print(f\"Wrote combined CSV to {output_path}\")\n",
        "\n",
        "data, composer_label_dict, index_composer_dict = prepare_data(input_csv, augmentations=True)\n",
        "\n",
        "prepared_data = data['sequence_vector']\n",
        "prepared_labels = data['label']\n",
        "\n",
        "#Uncomment to do k-fold cross validation instead\n",
        "# splits = cross_validation_splits(prepared_data, prepared_labels, max_folds=10)\n",
        "# all_fold_data = build_fold_data(prepared_data, prepared_labels, splits)\n",
        "# export_all_folds(all_fold_data)\n",
        "\n",
        "data_train, data_test, labels_train, labels_test, data_val, labels_val = split_data(prepared_data, prepared_labels)\n",
        "\n",
        "def make_df(Data, labels, split_name):\n",
        "    df = pd.DataFrame({\n",
        "        'sequence_vector': Data.values,\n",
        "        'label':            labels.values,\n",
        "    })\n",
        "    df['split'] = split_name\n",
        "    return df\n",
        "\n",
        "df_train = make_df(data_train, labels_train, 'train')\n",
        "df_test  = make_df(data_test , labels_test , 'test')\n",
        "df_val   = make_df(data_val  , labels_val  , 'validation')\n",
        "\n",
        "df_all = pd.concat([df_train, df_test, df_val], ignore_index=True)\n",
        "# df_all.to_csv('maestro_new_splits.csv', index=False)\n",
        "output_file = \"maestro_new_splits_augmented_403010_bigger_vocab.csv\"\n",
        "chunk_size = 2500\n",
        "\n",
        "\n",
        "with open(output_file, 'w', newline='') as f:\n",
        "    for start in tqdm(range(0, len(df_all), chunk_size), desc=\"Writing CSV\"):\n",
        "        end   = start + chunk_size\n",
        "        chunk = df_all.iloc[start:end]\n",
        "        chunk.to_csv(f, index=False, header=(start == 0))"
      ],
      "metadata": {
        "id": "-OJ4_jZe6RNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r data.zip /content/maestro_token_sequences_bigger_vocab.csv"
      ],
      "metadata": {
        "id": "j-5_W-IjUocs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pretty_midi\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "#Function to checkand make sure that tokenization can be reversed\n",
        "\n",
        "def verify(pm: pretty_midi.PrettyMIDI,\n",
        "                    token_dict: dict,\n",
        "                    inverse_token_dict: dict,\n",
        "                    time_step: float = 0.01,\n",
        "                    max_shift_secs: float = 1.0,\n",
        "                    num_velocity_bins: int = 32,\n",
        "                    tol_time: float = 0.01,\n",
        "                    fs_audio: int = 44100):\n",
        "\n",
        "    #reate velocity bins\n",
        "    vel_bins = np.linspace(0, 127, num_velocity_bins)\n",
        "\n",
        "    #collect original events\n",
        "    orig = []\n",
        "    for inst in pm.instruments:\n",
        "        for note in inst.notes:\n",
        "            orig.append((\"on\",  note.start, note.pitch, note.velocity))\n",
        "            orig.append((\"off\", note.end,   note.pitch, note.velocity))\n",
        "    orig.sort(key=lambda e: (e[1], e[0], e[2]))\n",
        "\n",
        "    #tokenize continuously\n",
        "    seq = [token_dict[\"<bos>\"]]\n",
        "    prev_t = 0.0\n",
        "    max_bin = int(max_shift_secs / time_step)\n",
        "\n",
        "    for typ, t, p, v in orig:\n",
        "        # emit time-shift tokens\n",
        "        dt = t - prev_t\n",
        "        steps = int(round(dt / time_step))\n",
        "        for i in range(steps):\n",
        "            b = min(i+1, max_bin)\n",
        "            seq.append(token_dict[f\"time_shift_{b}\"])\n",
        "        prev_t += steps * time_step\n",
        "\n",
        "        if typ == \"on\":\n",
        "            b = int(np.argmin(np.abs(vel_bins - v)))\n",
        "            seq.append(token_dict[f\"velocity_{b}\"])\n",
        "            seq.append(token_dict[f\"note_on_pitch_{p}\"])\n",
        "        else:\n",
        "            seq.append(token_dict[f\"note_off_pitch_{p}\"])\n",
        "\n",
        "    seq.append(token_dict[\"<eos>\"])\n",
        "\n",
        "    #decode back into event list\n",
        "    decoded = []\n",
        "    cur_t    = 0.0\n",
        "    last_vel = None\n",
        "\n",
        "    for tok in seq:\n",
        "        name = inverse_token_dict[tok]\n",
        "\n",
        "        if name.startswith(\"time_shift_\"):\n",
        "            # each one is exactly one time_step\n",
        "            cur_t += time_step\n",
        "\n",
        "        elif name.startswith(\"velocity_\"):\n",
        "            # velocity_{b}, where b was the bin index 0…31\n",
        "            b = int(name.split(\"_\")[-1])\n",
        "            last_vel = vel_bins[b]\n",
        "\n",
        "        elif name.startswith(\"note_on_pitch_\"):\n",
        "            p = int(name.split(\"_\")[-1])\n",
        "            # emit a note‐on with the most recent velocity\n",
        "            decoded.append((\"on\", cur_t, p, last_vel))\n",
        "\n",
        "        elif name.startswith(\"note_off_pitch_\"):\n",
        "            p = int(name.split(\"_\")[-1])\n",
        "            decoded.append((\"off\", cur_t, p, None))\n",
        "\n",
        "\n",
        "    #verify matching\n",
        "    unmatched_orig = orig.copy()\n",
        "    unmatched_dec  = decoded.copy()\n",
        "    matches = []\n",
        "\n",
        "    for o in orig:\n",
        "        for d in unmatched_dec:\n",
        "            if (o[0] == d[0] and\n",
        "                o[2] == d[2] and\n",
        "                abs(o[1] - d[1]) <= tol_time):\n",
        "                matches.append((o, d))\n",
        "                unmatched_orig.remove(o)\n",
        "                unmatched_dec.remove(d)\n",
        "                break\n",
        "\n",
        "    print(f\"Original events: {len(orig)}\")\n",
        "    print(f\"Decoded events:  {len(decoded)}\")\n",
        "    print(f\"Matches:         {len(matches)}\")\n",
        "    if unmatched_orig:\n",
        "        print(\"Missing matches for these original events (up to 5):\")\n",
        "        for e in unmatched_orig[:5]: print(\"   \", e)\n",
        "    if unmatched_dec:\n",
        "        print(\"Spurious decoded events (up to 5):\")\n",
        "        for e in unmatched_dec[:5]: print(\"   \", e)\n",
        "\n",
        "    success = (len(unmatched_orig) == 0 and len(unmatched_dec) == 0)\n",
        "\n",
        "    #reconstruct a new PrettyMIDI with decoded notes\n",
        "    new_pm = pretty_midi.PrettyMIDI()\n",
        "    piano  = pretty_midi.Instrument(program=0)  #piano\n",
        "    note_on_map = {}\n",
        "\n",
        "    for typ, t, p, v in decoded:\n",
        "        if typ == \"on\":\n",
        "            note_on_map[p] = (t, v or 100)  # fallback velocity\n",
        "        else:\n",
        "            if p in note_on_map:\n",
        "                s, vel = note_on_map.pop(p)\n",
        "                note = pretty_midi.Note(velocity=int(vel),\n",
        "                                        pitch=p,\n",
        "                                        start=s,\n",
        "                                        end=t)\n",
        "                piano.notes.append(note)\n",
        "    new_pm.instruments.append(piano)\n",
        "    return success"
      ],
      "metadata": {
        "id": "rgh2RZnhVoPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pretty_midi import PrettyMIDI\n",
        "\n",
        "#buld token dictionary\n",
        "token_dict, inverse_token_dict = generate_token_dict()\n",
        "\n",
        "#test with a single midi file\n",
        "pm = PrettyMIDI(\"/content/drive/MyDrive/maestro-v2.0.0/validation/ORIG-MIDI_03_7_8_13_Group__MID--AUDIO_18_R2_2013_wav--3.midi\")\n",
        "\n",
        "#verify\n",
        "ok = verify(pm, token_dict, inverse_token_dict)\n",
        "print(\"round-trip OK!\" if ok else \"problems detected\")"
      ],
      "metadata": {
        "id": "WMEXvANDYQ0a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
