{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "mdZlKX0m42z3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class MIDIVAE(nn.Module):\n",
        "  def __init__(self, encoder,decoder,classifier,device = 'cuda'):\n",
        "      super(MIDIVAE,self).__init__()\n",
        "\n",
        "\n",
        "      # untrained components\n",
        "      self.encoder = encoder\n",
        "      self.decoder = decoder\n",
        "\n",
        "      # pre-trained classifier\n",
        "      self.classifer = classifier\n",
        "\n",
        "\n",
        "      self.device = device\n",
        "\n",
        "  def forward(self, x,label):\n",
        "    #obtain latent space\n",
        "    z, mean,logvariance = self.encoder(x)\n",
        "    # concatenate x label with z\n",
        "    z = torch.cat([z,label.to(self.device)])\n",
        "    #feed through decoder\n",
        "    recon_midi = self.decoder(z)\n",
        "\n",
        "    #feed through classifier\n",
        "    composer_pred = self.classifier(recon_midi)\n",
        "\n",
        "    return recon_midi,mean,logvariance,composer_pred\n",
        "\n",
        "  def train(self, dataloader, optimizer, epochs=10, device='cuda'):\n",
        "    self.train()\n",
        "    # load data into some kind of trainloader?\n",
        "\n",
        "    # for loop with loading data for each batch\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x, label, composer in dataloader:  # assume dataloader returns inputs, labels, composers\n",
        "            x, label, composer = x.to(device), label.to(device), composer.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # call forward and get back the reconstructions, mean, logvariance, and composer prediction\n",
        "\n",
        "            recon_x, mu, logvar, pred = self.forward(x, label)\n",
        "\n",
        "            # do KLD with mean, logvariance and our prior N(0,I), backprop to encoder only\n",
        "            kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
        "\n",
        "            # do MSE with reconstructions and original data, backprop to encoder & decoder\n",
        "\n",
        "            recon_loss = F.mse_loss(recon_x, x)\n",
        "\n",
        "            # do CrossEntropy with data labels and composer predictions, backprop to decoder only\n",
        "\n",
        "            ce_loss = F.cross_entropy(pred, composer)\n",
        "\n",
        "            loss = recon_loss + kld + ce_loss  # optionally weigh each\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n",
        "\n",
        "  def generate(self,label):\n",
        "    # put on test mode\n",
        "    self.eval()\n",
        "    # generate normal gaussian noise\n",
        "    batch_size = label.size(0)\n",
        "    z = torch.randn(batch_size, self.encoder.latent_proj.fc_mu.out_features).to(self.device)\n",
        "    # concatenate noise with label\n",
        "    z_cond = torch.cat([z, label.to(self.device)], dim=1)\n",
        "    # feed through decoder\n",
        "\n",
        "    recon = self.decoder(z_cond)\n",
        "    return recon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token Embedding + Positional Encoding\n",
        "           ↓\n",
        "  Transformer Encoder\n",
        "           ↓\n",
        "     Sequence Embeddings (B, T, D)\n",
        "           ↓\n",
        "    Mean Pool / CLS Token → (B, D)\n",
        "           ↓\n",
        "      Linear → mu (B, latent_dim)\n",
        "      Linear → logvar (B, latent_dim)\n",
        "           ↓\n",
        "Reparameterization Trick: z = mu + eps * std\n",
        "\n",
        "label → Embedding → (B, label_dim)\n",
        "z     → (B, latent_dim)\n",
        "\n",
        "→ concat [z, label_emb] → (B, latent_dim + label_dim)\n",
        "→ Linear projection → (B, decoder_dim)  # matches decoder embedding dim\n",
        "→ z_cond\n",
        "\n",
        "[BOS, ..., tokens[:t-1]] → token embedding + pos encoding → (B, T', D)\n",
        "\n",
        "z_cond → unsqueeze(1) → broadcast across T' → (B, T', D)\n",
        "\n",
        "decoder_input = token_emb + z_cond\n",
        "\n",
        "Transformer Decoder → token_logits (B, T', vocab_size)\n"
      ],
      "metadata": {
        "id": "aAT6hAdgFt_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Token_Embedding(nn.Module):\n",
        "  def __init__(self,vocab_size,embedding_dim):\n",
        "    super(Token_Embedding,self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.embedding(x)\n",
        "\n",
        "class Pos_Embedding(nn.Module):\n",
        "  def __init__(self,max_len,embedding_dim):\n",
        "    super(Pos_Embedding,self).__init__()\n",
        "    self.pos_embedding = nn.Embedding(max_len,embedding_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    seq_len = x.size(1)\n",
        "    pos_ids = torch.arange(seq_len,device=x.device).unsqueeze(0)\n",
        "    return self.pos_embedding(pos_ids)\n",
        "\n",
        "class Transformer_Encoder(nn.Module):\n",
        "  def __init__(self,embedding_dim,num_heads,num_layers,ff_dim):\n",
        "    super(Transformer_Encoder,self).__init__()\n",
        "    self.self_attention = nn.MultiheadAttention(embedding_dim,num_heads)\n",
        "    self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "    self.ffn = nn.Sequential(nn.Linear(embedding_dim,ff_dim),nn.ReLU(),nn.Linear(ff_dim,embedding_dim))\n",
        "\n",
        "  def forward(self,x):\n",
        "    print(x.shape)\n",
        "    # multihead attention is dumb and doesnt like batch size first\n",
        "    x = x.permute(1, 0, 2)\n",
        "    output,_ = self.self_attention(x,x,x)\n",
        "    # maybe add normalization\n",
        "\n",
        "    ffn_output = self.ffn(x)\n",
        "    x = self.layer_norm(x + ffn_output)\n",
        "    #undo what we did\n",
        "    x = x.permute(1, 0, 2)\n",
        "    return x\n",
        "\n",
        "class LatentSpace_Mean_Log(nn.Module):\n",
        "  def __init__(self,embedding_dim,latent_dim):\n",
        "    super(LatentSpace_Mean_Log,self).__init__()\n",
        "    self.fc_mu = nn.Linear(embedding_dim,latent_dim)\n",
        "    self.fc_logvar = nn.Linear(embedding_dim,latent_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    mu = self.fc_mu(x)\n",
        "    logvar = self.fc_logvar(x)\n",
        "\n",
        "    return mu,logvar"
      ],
      "metadata": {
        "id": "rSBeiQ44HXNq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Variational_Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, max_len, latent_dim, num_heads, num_layers, ff_dim):\n",
        "      super(Variational_Encoder,self).__init__()\n",
        "\n",
        "      self.token_embedding = Token_Embedding(vocab_size,embedding_dim)\n",
        "      self.pos_embedding = Pos_Embedding(max_len,embedding_dim)\n",
        "      self.encoder = Transformer_Encoder(embedding_dim,num_heads,num_layers,ff_dim)\n",
        "      self.latent_proj = LatentSpace_Mean_Log(embedding_dim,latent_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # create embeddings\n",
        "    tok_embeddings = self.token_embedding(x)\n",
        "    pos_embeddings = self.pos_embedding(x)\n",
        "    embeddings = pos_embeddings + tok_embeddings\n",
        "\n",
        "    # obtain output\n",
        "    output = self.encoder(embeddings)\n",
        "\n",
        "    # pool it up!!\n",
        "    pooled_output = output.mean(dim=1)\n",
        "\n",
        "    # project into latent space and reparameterize\n",
        "    mean,logvariance = self.latent_proj(pooled_output)\n",
        "    std = torch.exp(0.5 * logvariance)\n",
        "    eps = torch.randn_like(std)\n",
        "    z = mean + eps * std\n",
        "\n",
        "    return z, mean,logvariance\n"
      ],
      "metadata": {
        "id": "Wn_47aTe66-3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of instantiation of it\n",
        "\n",
        "vocab_size = 512        # Number of unique MIDI tokens\n",
        "embedding_dim = 256     # Size of token embeddings\n",
        "max_len = 512          # Max sequence length (you can adjust this)\n",
        "latent_dim = 128       # Latent space dimension\n",
        "num_heads = 8          # Number of attention heads\n",
        "num_layers = 6         # Number of transformer layers\n",
        "ff_dim = 512           # Feed-forward layer dimension\n",
        "\n",
        "encoder = Variational_Encoder(vocab_size, embedding_dim, max_len, latent_dim, num_heads, num_layers, ff_dim)\n",
        "\n",
        "# Example input (batch of MIDI token sequences)\n",
        "x = torch.randint(0, vocab_size, (32, 100))  # Batch size 32, sequence length 100\n",
        "\n",
        "z, mu, logvar = encoder(x)\n",
        "print(f\"z shape: {z.shape}, mu shape: {mu.shape}, logvar shape: {logvar.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdooHH7WGrsY",
        "outputId": "2a8fbc5d-7c80-4560-de3b-4571972b7f61"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100, 256])\n",
            "z shape: torch.Size([32, 128]), mu shape: torch.Size([32, 128]), logvar shape: torch.Size([32, 128])\n"
          ]
        }
      ]
    }
  ]
}