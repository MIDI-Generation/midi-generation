{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdZlKX0m42z3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class MIDIVAE(nn.Module):\n",
        "  def __init__(self, encoder,vector_quantizer, decoder,classifier,device = 'cuda'):\n",
        "      super(MIDIVAE,self).__init__()\n",
        "\n",
        "\n",
        "      # untrained components\n",
        "      self.encoder = encoder\n",
        "      self.vector_quantizer = vector_quantizer  # Vector Quantized VAE to discretize the latent space\n",
        "      self.decoder = decoder  # Cross-Attention transformer to generate MIDI vectors\n",
        "\n",
        "      # pre-trained classifier\n",
        "      self.classifer = classifier\n",
        "\n",
        "\n",
        "      self.device = device\n",
        "\n",
        "  def forward(self, x,label):\n",
        "    #obtain latent space\n",
        "    z, mean,logvariance = self.encoder(x)\n",
        "\n",
        "    # quantize the latent space\n",
        "    z, quantized, vq_loss = self.vector_quantizer(z)\n",
        "\n",
        "    # concatenate x label with z\n",
        "    z = torch.cat([z,label.to(self.device)])\n",
        "    #feed through decoder\n",
        "    recon_midi = self.decoder(z)\n",
        "\n",
        "    #feed through classifier\n",
        "    composer_pred = self.classifier(recon_midi)\n",
        "\n",
        "    return recon_midi,mean,logvariance,composer_pred, vq_loss\n",
        "\n",
        "  def train_model(self, dataloader, optimizer, epochs=10, device='cuda'):\n",
        "    self.train()\n",
        "    # load data into some kind of trainloader?\n",
        "\n",
        "    # for loop with loading data for each batch\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x, label, composer in dataloader:  # assume dataloader returns inputs, labels, composers\n",
        "            x, label, composer = x.to(device), label.to(device), composer.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # call forward and get back the reconstructions, mean, logvariance, and composer prediction\n",
        "\n",
        "            recon_x, mu, logvar, pred, vq_loss = self.forward(x, label)\n",
        "\n",
        "            # do KLD with mean, logvariance and our prior N(0,I), backprop to encoder only\n",
        "            kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
        "\n",
        "            #! Connor - I made changes here                         <------\n",
        "            # compute cross-entropy loss between the model’s next-token logits (dropping the last step) and the true next tokens (dropping the first)\n",
        "            tgt = x[:, 1:].transpose(0, 1)\n",
        "            logits = recon_x[:, :-1]\n",
        "            recon_loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)),tgt.reshape(-1)) \n",
        "\n",
        "            # do CrossEntropy with data labels and composer predictions, backprop to decoder only\n",
        "\n",
        "            ce_loss = F.cross_entropy(pred, composer)\n",
        "\n",
        "            loss = recon_loss + kld + ce_loss + vq_loss  # optionally weigh each\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n",
        "\n",
        "  def generate(self,label):\n",
        "    # put on test mode\n",
        "    self.eval()\n",
        "    # generate normal gaussian noise\n",
        "    batch_size = label.size(0)\n",
        "    z = torch.randn(batch_size, self.encoder.latent_proj.fc_mu.out_features).to(self.device)\n",
        "    # concatenate noise with label\n",
        "    z_cond = torch.cat([z, label.to(self.device)], dim=1)\n",
        "    # feed through decoder\n",
        "\n",
        "    recon = self.decoder(z_cond)\n",
        "    return recon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAT6hAdgFt_t"
      },
      "source": [
        "Token Embedding + Positional Encoding\n",
        "           ↓\n",
        "  Transformer Encoder\n",
        "           ↓\n",
        "     Sequence Embeddings (B, T, D)\n",
        "           ↓\n",
        "    Mean Pool / CLS Token → (B, D)\n",
        "           ↓\n",
        "      Linear → mu (B, latent_dim)\n",
        "      Linear → logvar (B, latent_dim)\n",
        "           ↓\n",
        "Reparameterization Trick: z = mu + eps * std\n",
        "\n",
        "label → Embedding → (B, label_dim)\n",
        "z     → (B, latent_dim)\n",
        "\n",
        "→ concat [z, label_emb] → (B, latent_dim + label_dim)\n",
        "→ Linear projection → (B, decoder_dim)  # matches decoder embedding dim\n",
        "→ z_cond\n",
        "\n",
        "[BOS, ..., tokens[:t-1]] → token embedding + pos encoding → (B, T', D)\n",
        "\n",
        "z_cond → unsqueeze(1) → broadcast across T' → (B, T', D)\n",
        "\n",
        "decoder_input = token_emb + z_cond\n",
        "\n",
        "Transformer Decoder → token_logits (B, T', vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rSBeiQ44HXNq"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Token_Embedding(nn.Module):\n",
        "  def __init__(self,vocab_size,embedding_dim):\n",
        "    super(Token_Embedding,self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.embedding(x)\n",
        "\n",
        "class Pos_Embedding(nn.Module):\n",
        "  def __init__(self,max_len,embedding_dim):\n",
        "    super(Pos_Embedding,self).__init__()\n",
        "    self.pos_embedding = nn.Embedding(max_len,embedding_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    seq_len = x.size(1)\n",
        "    pos_ids = torch.arange(seq_len,device=x.device).unsqueeze(0)\n",
        "    return self.pos_embedding(pos_ids)\n",
        "\n",
        "class Transformer_Encoder(nn.Module):\n",
        "  def __init__(self,embedding_dim,num_heads,num_layers,ff_dim):\n",
        "    super(Transformer_Encoder,self).__init__()\n",
        "    self.self_attention = nn.MultiheadAttention(embedding_dim,num_heads)\n",
        "    self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "    self.ffn = nn.Sequential(nn.Linear(embedding_dim,ff_dim),nn.ReLU(),nn.Linear(ff_dim,embedding_dim))\n",
        "\n",
        "  def forward(self,x):\n",
        "    print(x.shape)\n",
        "    # multihead attention is dumb and doesnt like batch size first\n",
        "    x = x.permute(1, 0, 2)\n",
        "    output,_ = self.self_attention(x,x,x)\n",
        "    # maybe add normalization\n",
        "\n",
        "    ffn_output = self.ffn(x)\n",
        "    x = self.layer_norm(x + ffn_output)\n",
        "    #undo what we did\n",
        "    x = x.permute(1, 0, 2)\n",
        "    return x\n",
        "\n",
        "class LatentSpace_Mean_Log(nn.Module):\n",
        "  def __init__(self,embedding_dim,latent_dim):\n",
        "    super(LatentSpace_Mean_Log,self).__init__()\n",
        "    self.fc_mu = nn.Linear(embedding_dim,latent_dim)\n",
        "    self.fc_logvar = nn.Linear(embedding_dim,latent_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    mu = self.fc_mu(x)\n",
        "    logvar = self.fc_logvar(x)\n",
        "\n",
        "    return mu,logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Wn_47aTe66-3"
      },
      "outputs": [],
      "source": [
        "class Variational_Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, max_len, latent_dim, num_heads, num_layers, ff_dim):\n",
        "      super(Variational_Encoder,self).__init__()\n",
        "\n",
        "      self.token_embedding = Token_Embedding(vocab_size,embedding_dim)\n",
        "      self.pos_embedding = Pos_Embedding(max_len,embedding_dim)\n",
        "      self.encoder = Transformer_Encoder(embedding_dim,num_heads,num_layers,ff_dim)\n",
        "      self.latent_proj = LatentSpace_Mean_Log(embedding_dim,latent_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # create embeddings\n",
        "    tok_embeddings = self.token_embedding(x)\n",
        "    pos_embeddings = self.pos_embedding(x)\n",
        "    embeddings = pos_embeddings + tok_embeddings\n",
        "\n",
        "    # obtain output\n",
        "    output = self.encoder(embeddings)\n",
        "\n",
        "    # pool it up!!\n",
        "    pooled_output = output.mean(dim=1)\n",
        "\n",
        "    # project into latent space and reparameterize\n",
        "    mean,logvariance = self.latent_proj(pooled_output)\n",
        "    std = torch.exp(0.5 * logvariance)\n",
        "    eps = torch.randn_like(std)\n",
        "    z = mean + eps * std\n",
        "\n",
        "    return z, mean,logvariance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "    \"\"\"\n",
        "    Vector Quantizer implements the VQ‐VAE codebook lookup and loss.\n",
        "    Args:\n",
        "      num_embeddings: number of codebook vectors (K)\n",
        "      embedding_dim:   dimensionality of each vector (D)\n",
        "      commitment_cost: hyperparameter that pushes the encoder to stay close to their codebook vectors\n",
        "    \"\"\"\n",
        "    def __init__(self, num_embeddings, embedding_dim = 512, commitment_cost = 0.25):\n",
        "        super().__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim   = embedding_dim\n",
        "        self.commitment_cost = commitment_cost\n",
        "\n",
        "        # Codebook: K × D, initialized uniformly\n",
        "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / num_embeddings, 1.0 / num_embeddings)\n",
        "    \n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        z: continuous latent vectors from encoder, shape [B, D]\n",
        "        returns:\n",
        "          encoding_indices: [B] discrete tokens we are feeding to the transformer decoder\n",
        "          quantized: [B, D] embedding vectors for each token\n",
        "          vq_loss: scalar loss\n",
        "        \"\"\"\n",
        "        # get batch-size and latent dimension  \n",
        "        B, D = z.shape\n",
        "        assert D == self.embedding_dim, f\"Expected latent_dim={self.embedding_dim}, but got {D}\"\n",
        "\n",
        "        # prep for distance calculation\n",
        "        z_flat = z.view(B, D) # ensure z is [B, D]\n",
        "        emb_w   = self.embedding.weight # our codebook matrix, shape [K, D]\n",
        "\n",
        "        # compute squared L2 distances: ||z - e_k ||^2\n",
        "        z_sq = torch.sum(z_flat**2, dim=1, keepdim=True) # [B, 1], each ||z ||^2\n",
        "        e_sq = torch.sum(emb_w**2, dim=1) # [K], each ||e_k||^2\n",
        "        dist = z_sq + e_sq.unsqueeze(0) - 2 * (z_flat @ emb_w.t())  # [B, K], dot(z, e_k')\n",
        "\n",
        "        # find nearest code for each z\n",
        "        encoding_indices = torch.argmin(dist, dim=1) # [B]\n",
        "\n",
        "        # convert to one-hot encoding\n",
        "        encodings = F.one_hot(encoding_indices, self.num_embeddings).type(z.dtype)  # [B, K]\n",
        "\n",
        "        # quantized vectors = one_hot encodings @ codebook\n",
        "        quantized = encodings @ emb_w # [B, D]\n",
        "        quantized = quantized.view_as(z)\n",
        "\n",
        "        # compute VQ losses\n",
        "        # codebook loss: move embeddings toward encoder outputs\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), z)\n",
        "        # commitment loss: move encoder outputs toward embeddings\n",
        "        q_latent_loss = F.mse_loss(quantized, z.detach())\n",
        "        vq_loss = e_latent_loss + self.commitment_cost * q_latent_loss\n",
        "\n",
        "        # straight-through: allow gradients to flow to z\n",
        "        quantized = z + (quantized - z).detach()\n",
        "\n",
        "        return encoding_indices, quantized, vq_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!!! Adapted from HW5!!!\n",
        "\n",
        "from torch import nn, Tensor\n",
        "import torch\n",
        "import math\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Cross-Attention Transformer Decoder\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 src_vocab_size: int, \n",
        "                 tgt_vocab_size: int, \n",
        "                 d_model: int, \n",
        "                 nhead: int, \n",
        "                 d_hid: int,\n",
        "                 nlayers: int, \n",
        "                 dropout: float = 0.5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)  # discrete latent tokens\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)  # predicted MIDI tokens\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        # Decoder (self-attention and cross-attention)\n",
        "        dec_layer = TransformerDecoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_decoder = TransformerDecoder(dec_layer, nlayers)\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.src_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.tgt_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, \n",
        "                tgt: Tensor,\n",
        "                memory: Tensor,\n",
        "    ): \n",
        "        \"\"\"\n",
        "        Args:\n",
        "          tgt: [tgt_seq_len, batch_size] - the target sequence to predict the next token of (discrete MIDI tokens)\n",
        "          memory: [src_seq_len, batch_size] - the discrete VQ-VAE code indices\n",
        "        Returns:\n",
        "          [tgt_seq_len, batch_size, tgt_vocab_size] logits output for the next token\n",
        "        \"\"\"\n",
        "        \n",
        "        src = self.src_embedding(memory) * math.sqrt(self.d_model) # Scale and Embed the source sequence\n",
        "        src = self.pos_encoder(src) # Add positional encoding\n",
        "\n",
        "        tgt = self.tgt_embedding(tgt) * math.sqrt(self.d_model) # Scale and Embed the target sequence\n",
        "        tgt = self.pos_encoder(tgt) # Add positional encoding\n",
        "\n",
        "\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(len(tgt)).to(device)  # Create the mask\n",
        "        output = self.transformer_decoder(tgt=tgt, memory=src, tgt_mask=tgt_mask)  # Pass them through the transformer\n",
        "        output = self.linear(output)  # Apply the linear layer\n",
        "        return output\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdooHH7WGrsY",
        "outputId": "2a8fbc5d-7c80-4560-de3b-4571972b7f61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 100, 256])\n",
            "z shape: torch.Size([32, 128]), mu shape: torch.Size([32, 128]), logvar shape: torch.Size([32, 128])\n"
          ]
        }
      ],
      "source": [
        "# example of instantiation of it\n",
        "\n",
        "vocab_size = 512        # Number of unique MIDI tokens\n",
        "embedding_dim = 256     # Size of token embeddings\n",
        "max_len = 512          # Max sequence length (you can adjust this)\n",
        "latent_dim = 128       # Latent space dimension\n",
        "num_heads = 8          # Number of attention heads\n",
        "num_layers = 6         # Number of transformer layers\n",
        "ff_dim = 512           # Feed-forward layer dimension\n",
        "\n",
        "encoder = Variational_Encoder(vocab_size, embedding_dim, max_len, latent_dim, num_heads, num_layers, ff_dim)\n",
        "\n",
        "# Example input (batch of MIDI token sequences)\n",
        "x = torch.randint(0, vocab_size, (32, 100))  # Batch size 32, sequence length 100\n",
        "\n",
        "z, mu, logvar = encoder(x)\n",
        "print(f\"z shape: {z.shape}, mu shape: {mu.shape}, logvar shape: {logvar.shape}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
